# ======================================================================
# Customizing the config
#
# Do not edit this file, so that it will stay synced with what's in the
# git archive.  (The tests also depend on what's in here.)  To change
# values, the YAML files given in "overrides" will replace any values
# here with what's there.  The YAML files given in "augments" will not
# replace anything, but if there are new values (e.g. in dictionaries)
# that aren't present here, they will be added.
#
# WARNING : python's yaml reading is broken.  It will read 1e10 as a
# string, not a float.  Write 1.0e+10 to make it work right.  There are
# two things here: the first is the + after e (which, I think *is* part
# of the yaml spec, even though we can freely omit that + in Python and
# C).  The second is the decimal point; the YAML spec says it's not
# necessary, but python won't recognize it as a float without it.

overrides:
  - local_overrides.yaml
augments:
  - local_augments.yaml

# ======================================================================
# Paths

path:
  data_root: null
  data_temp: null

# ======================================================================
# Databse config

db:
  engine: postgresql
  user: postgres
  password: fragile
  password_file: null
  host: localhost
  port: 5432
  database: seechange

# ======================================================================
# Local file storage
#
# format: string
#   Format for image files.  Currently only "fits" and "fitsfz" are
#   supported.  We have vague ambitions to support HDF5 files.  ASDF has
#   only very vaguely been considered by one person.
#
# single_file: bool
#   If True, then weight, flags will be stored in the same file with the
#   image data (as different HDU extensions for FITS files).  If False,
#   then image, weight, flags are three separate FITS files.
#   single_file=True is not really fully supported at the moment, and
#   would need some development and a bunch of thought before one should
#   trust using it.
#
# name_convention: string
#   Use any of the following: inst_name, date, time, section_id, filter,
#   ra, dec, prov_id.  Can also use section_id_int if the section_id is
#   always an integer.  Can also use ra_int and ra_frac to get the integer
#   number before/after the decimal point (the same can be done for
#   dec).  Also use ra_int_h to get the number in hours, etc.  To get the
#   declination with "p" or "m" replacing the sign, use dec_int_pm.  The
#   string given here is fed into the python format() function so you
#   can use e.g., {ra_int:03d} to get a 3 digit zero padded right
#   ascension.  The name convention can also include subfolders (e.g.,
#   using {ra_int}/...).  The minimal set of fields to make the
#   filenames unique include: inst_name (short instrument name), date,
#   time, section_id, prov_hash (in this example, the first six
#   characters of the provenance unique hash)

storage:
  images:
    format: fits
    single_file: false
    name_convention: "{ra_int:03d}/{inst_name}_{date}_{time}_{section_id}_{filter}_{im_type}_{prov_hash:.6s}"


# ======================================================================
# Archive:
#
# Set to null if there is no archive; otherwise, a dict
#
# Fields of the dict:
#
#   archive_url: string or null
#     The URL of the archive server, or null if archive is on the filesystem
#
#   verify_cert: bool
#     Should we verify the SSL cert of the archive server?  You usually
#     want this to be true, but at least for testing we set this to
#     false so we don't have to deal with getting certificates for
#     our internally contained docker environment.
#
#   path_base: string
#     The base of the collection on the archive server (a string unique
#     to this dataset).  (This is not a full path, but relative to the
#     base of the overall archive, and will usually just be a single
#     word.)
#
#   local_read_dir: string or null
#     The directory to read from if the archive is on the local
#     filesystem, or null.  Different from local_write_dir in case
#     the system has a faster read-only method of accessing the same
#     files (which is the case on NERSC).  If null, then the archive
#     is not accessible locally, and the archive_url server will be
#     used instead.  May be the same as local_write_dir.
#
#   local_write_dir: string or null
#     The directory to write to if the archive is on the local
#     filesystem, or null if we need to use archive_url.

archive: null


# ======================================================================
# Conductor
#
# The conductor is the service that the pipeline uses to figure out what
# exposures are available and need processing.
#
# (There will be other values set in a config file used by the actual
# conductor.)
#
#   conductor_url: string
#     The default value here is what we use in our test environment.
#     It's almost certainly wrong for any production config.
#
#   username: string
#     The username for authentication on the conductor.
#
#   password: string
#     The password for authentication on the conductor.
#
#   password_file: string
#     A file containing the password for authentication on the
#     conductor.  Set password to null and then set this if
#     you have a secure "secrets" directory where you keep passwords,
#     so that you don't have to put the password in an actual
#     config file that might be publicly readable or that might
#     get committed to a git archive.
#
#   flask_secret_key:
#   flask_secret_key_file:
#     These are only used by the conductor itself, and can
#     be safely left as null for the main pipeline.  However,
#     in the conductor config (conductor/seechange_conductor.yaml
#     and conductor/local_overrides.yaml), this must be set to
#     something real.

conductor:
  conductor_url: https://conductor:8082/
  username: unknown
  password: null
  password_file: null
  flask_secret_key: null
  flask_secret_key_file: null

# ======================================================================
# Webap
#
# This config is not used by the main pipeline, only the wbap.
# When building the Dockerfile, those configs will be set from
# webap/seechange_webap.yaml and webap/local_overrides.yaml
# (though they include this file)

webap:
  webap_url: https://webap:8081/
  flask_secret_key: null
  flask_secret_key_file: null

# ======================================================================
# Email for Conductor and Webap
#
# These configs are only used by the conductor and webap; their
# values do not matter for the main pipeline.  However, they must
# be set properly in the config files used when building the Docker
# image for the conductor and the webap.  The configs below are what's
# needed for our test environment, and are definitely wrong for any
# production environment.

email:
  email_from: 'Seechange <nobody@nowhere.org>'
  email_subject: 'Seechange password reset'
  email_system_name: 'Seechange'
  smtp_server: 'mailhog'
  smtp_port: 1025
  smtp_use_ssl: false
  smtp_username: null
  smtp_password: null
  smtp_password_file: null


# ======================================================================
# Gaia DR3 server
#
# Gaia stars are used for both astrometric and photometric
# calibration.  (As this is a discovery pipeline, we don't
# promise photometric calibration to better than a few %.)
#
# There are two ways we can get it : through the server defined in the
#  submodule extern/nersc-desi-gaia-dr3-server, and via NOIRLab
#  using their queryClient.
#
#   use_server: bool
#      If true, use the custom gaia server.
#
#   server_url: string
#      The url of the custom server
#
#   fallback_datalab: bool
#      If use_server is False, or if user_server is True and the custom
#      server is unavailable or fails after five tries, then try falling
#      back to the NOIRLab server if fallback_datalab is True.

catalog_gaiadr3:
  # use_server: True
  # TEMPORARY 2025-01-27 : NERSC is down this week, so don't even try the spin server.
  use_server: False
  fallback_datalab: True
  server_url: https://ls4-gaia-dr3.lbl.gov
  server_timeout_sec: 5.


# ======================================================================
# ======================================================================
# ======================================================================
# Pipeline components
#

# ======================================================================
# Overall pipeline
#
# For additional documentation on the parameters, see the Parameters
# subclass in the file that defines each part of the pipeline
#
# save_before_subtraction : bool
#    If true, save images and data products after
#    initial reduction but before the pipeline looks for a reference
#    to make a subtraction
#
# save_at_finish : bool
#    Save everything at the end of the pipeline run.
#
# provenance_tag: string
#    The ProvenanceTag that products of the pipeline should be
#    associated with
#

pipeline:
  save_before_subtraction: true
  save_at_finish: true
  provenance_tag: current

# ======================================================================
# Preprocessing
#
# steps_required : list of str
#    Steps that must be done on the image.  Either they come with all
#    these steps done, or we must perform them.  (If they come with all
#    steps done, the ideally the pipeline will flag them as such when
#    ingesting them and won't incorrectly do something twice.)

preprocessing:
  steps_required: [ 'overscan', 'linearity', 'flat', 'fringe' ]

# ======================================================================
# Extraction
#
# Right now, "extraction" actually encompasses four steps.  Those steps
# are all done at once and so have a single provenance associated with
# them.  (This is the reason that all those steps' parameters are stored
# underneath extraction-- so that the provenance's parameters will take
# all of them into account.)  Those steps are backgrounding
# (i.e. finding the sky background) ("bg"), extraction of sources
# ("sources"), doing the astrometric solution ("wcs"), and doing the
# photometric calibration ("zp").  See the pipeline documentation for
# the meanings of all the parmeters.

extraction:
  sources:
    method: sextractor
    measure_psf: true
    apertures: [1.0, 2.0, 3.0, 5.0]
    inf_aper_num: -1
    best_aper_num: 0
    aperunit: fwhm
    separation_fwhm: 1.0
    threshold: 3.0
    subtraction: false
  bg:
    format: map
    method: sep
    poly_order: 1
    sep_box_size: 128
    sep_filt_size: 3
  wcs:
    cross_match_catalog: gaia_dr3
    solution_method: scamp
    max_catalog_mag: [22.0]
    mag_range_catalog: 6.0
    min_catalog_stars: 50
    max_sources_to_use: [2000, 1000, 500, 200]
  zp:
    cross_match_catalog: gaia_dr3
    max_catalog_mag: [22.0]
    mag_range_catalog: 6.0
    min_catalog_stars: 50

# ======================================================================
# Subtraction
#
# (The pipeline will also do reference finding, but the config for
# that is further down.)
#
# refset:
#    "set refset to null to only make references (no subtraction will
#     happen to start running subtractions, first make a ref set and use
#     the name of than. in this field." <-- TODO look a this comment.
#     It seems odd.  Look at the code, and see if this is really the
#     behavior.  It seems like weird behavior.  Better would be to add a
#     "steptotodo" parameter to the overall pipeline above.  Reference
#     generation should have its own executable, not be a side effect of
#     non-obvious parameter settings in the subtraction/discvoery
#     pipeline.

subtraction:
  method: zogy
  refset: null
  alignment_index: new
  alignment:
    method: swarp
  reference:
    minovfrac: 0.85
    must_match_instrument: true
    must_match_filter: true
    must_match_section: false
    must_match_target: false

# ======================================================================#
# Detection
#
# Finding sources on difference images
#
# subtraction : bool
#    This should always be true, because in the nomenclature of this
#    pipeline, "detection" is what we run on a difference images, and
#    "extraction" is what we run on science images, even though the
#    actual code run in both cases may well be the same.
#
# method : str
#    For the ZOGY subtraction method, the detection method must be
#    filter.  For the hotpants method, the detection method must be
#    sextractor.
#
# See pipline/subtraction.py for parameter definitions

detection:
  subtraction: true
  method: filter
  threshold: 5.0

# ======================================================================#
# Cutting
#
# Making new, ref, sub thumbnail cutouts.
#
# The RBbot models we're using have a predetermined cutout size, which
# must match what's done here.  The models we're using now require a
# 41×41 cutout.

cutting:
  cutout_size: 41

# ======================================================================
# Measuring
#
# Measuring fluxes and positions of detections, as well as making
# analytical cuts.
#
# If you want (for testing purposes) to have all measurements
# saved regardless of whether they're good or not, set all
# the deletion thresholds to null.  (A few measurements will
# still get filtered out, if most of the pixels in the region
# of the source is masked.)
#
# See pipeline/measuring.py for definition of the parameters

measuring:
  annulus_radii: [ 4., 5. ]
  annulus_units: fwhm
  use_annulus_bg_on_sub: False
  diag_box_halfsize: 2.
  diag_box_halfsize_unit: fwhm
  negatives_n_sigma_outlier: 2.
  bad_thresholds:
    psf_fit_flags_bitmask: 0x2e
    detection_dist: 5.
    gaussfit_dist: 5.
    elongation: 3.
    width_ratio: 2.
    nbadpix: 1
    negfrac: 0.3
    negfluxfrac: 0.3
  deletion_thresholds:
    psf_fit_flags_bitmask: 0x2e
    detection_dist: 5.
    gaussfit_dist: 5.
    elongation: 3.
    width_ratio: 2.
    nbadpix: 1
    negfrac: 0.3
    negfluxfrac: 0.3
    # psf_fit_flags_bitmask: null
    # detection_dist: null
    # gaussfit_dist: null
    # elongation: null
    # width_ratio: null
    # nbadpix: null
    # negfrac: null
    # negfluxfrac: null
  association_radius: 2.0
  do_not_associate: False


# ======================================================================
# Scoring
#
# This is for "deep scoring", i.e. (probably) deep learning methods for
# assigning an empirical score to the cutouts.

scoring:
  algorithm: RBbot-quiet-shadow-131-cut0.55
  rbbot_model_dir: /seechange/share/RBbot_models

# ======================================================================
# Alert generation
#
# Set "send_alerts" to false to completely disable alert generation.
#
# Alerts may go to multiple destinations, so the configuration includes
# a list of dicts with parameters.  Those parameters include at least
# 'method' and 'enabled'; method determines the actual code used to send
# the alert.  enabled indicates whether this method is currently
# used. deepcut is the scoring threshold (0-1) above which a detection
# will be sent out as an alert.  Further parameters are method dependent.
# The same method may show up more than once, e.g. if alerts are being
# sent to multiple kafka servers, or if alerts are sent to multiple
# different topics on the same kafka server with different parameters.
#
# 'deepcut': if Null, use the nominal value for the algorithm
# (in DeepScore.get_rb_cut).  If a float, use that.  This is the r/b
# cut below which no alert will be generated.
#
# kafka_topic_pattern : TODO DOCS

alerts:
  send_alerts: true
  methods:
    - name: filtered_kafka_stream
      method: kafka
      enabled: false
      deepcut: null
      avro_schema: /seechange/share/avsc/alert.avsc
      kafka_server: kafka.server.com
      kafka_topic_pattern: "topic_{year:04d}-{month:02d}-{day:02d}"
      previous_source_days: 365
    - name: junk_included_kafka_stream
      method: kafka
      enabled: false
      deepcut: 0.
      avro_schema: /seechange/share/avsc/alert.avsc
      kafka_server: kafka.server.com
      kafka_topic_pattern: "topic_with_junk_{year:04d}-{month:02d}-{day:02d}"
      previous_source_days: 365

# ======================================================================
# ======================================================================
# ======================================================================
# Coaddition pipeline
#
# pipeline:
#   date_range: number of days before end date to set the start date
#               when searching for images to caodd.  (TODO : is this used???)
#
# coaddition: see pipeline/coaddition.py
#
# extraction: These parameters override the extraction parameters
#             set above.  If nothing is here, then that
#             config will be used as is.


coaddition:
  pipeline:
    date_range: 7.0
  coaddition:
    method: zogy
    noise_estimator: sep
    flag_fwhm_factor: 1.0
    alignment_index: last
    alignment:
      method: swarp
    inpainting:
      multi_image_method: median
      feather_width: 2
      rescale_method: median
      single_image_method: biharmonic
      ignore_flags: 0
  extraction:
    sources:
      measure_psf: true
      threshold: 3.0
      method: sextractor
    bg:
      format: map
      method: sep
    wcs:
      cross_match_catalog: gaia_dr3
      solution_method: scamp
      max_catalog_mag: [22.0]
      mag_range_catalog: 6.0
      min_catalog_stars: 50
    zp:
      cross_match_catalog: gaia_dr3
      max_catalog_mag: [22.0]
      mag_range_catalog: 6.0
      min_catalog_stars: 50

# ======================================================================
# ======================================================================
# ======================================================================
# Reference making


# use these parameters to make references by coadding images
# the reference pipeline will load the regular configuration first,
# then the coaddition config, and only in the end, override with this:
referencing:
  maker:  # these handles the top level configuration (e.g., how to choose images that go into the reference maker)
    name: best_references  # the name of the reference set that you want to make references for
    description: "The Best References"   # Description of the reference set
    start_time: null  # only grab images after this time
    end_time: null  # only grab images before this time
    instrument: null  # only grab images from this instrument (if list, will make cross-instrument references)
    # filter: null  # only grab images with this filter
    project: null  # only grab images with this project
    min_airmass: null  # only grab images with airmass above this
    max_airmass: null  # only grab images with airmass below this
    min_background: null  # only grab images with background rms above this
    max_background: null  # only grab images with background rms below this
    min_seeing: null  # only grab images with seeing above this
    max_seeing: null  # only grab images with seeing below this
    min_lim_mag: null  # only grab images with limiting magnitude above this
    max_lim_mag: null  # only grab images with limiting magnitude below this
    min_exp_time: null  # only grab images with exposure time above this
    max_exp_time: null  # only grab images with exposure time below this
    min_number: 7  # only create a reference if this many images can be found
    max_number: 30  # do not add more than this number of images to the reference
    corner_distance: 0.8  # see the docs
    overlap_fraction: 0.9
    coadd_overlap_fraction: 0.1
    min_only_center: False
    seeing_quality_factor: 3.0  # linear coefficient for adding lim_mag and seeing to get the "image quality"
    save_new_refs: true  # should the new references be saved to disk and committed to the database?
  pipeline:  # The following are used to override the regular "extraction" parameters
    extraction:
      sources:  # override the regular source and psf extraction parameters
        measure_psf: true
#        threshold: 3.0
#        method: sextractor
#      bg:
#        format: map
#        method: sep
#        poly_order: 1
#        sep_box_size: 128
#        sep_filt_size: 3
#      wcs:  # override the regular astrometric calibration parameters
#        cross_match_catalog: gaia_dr3
#        solution_method: scamp
#        max_catalog_mag: [22.0]
#        mag_range_catalog: 6.0
#        min_catalog_stars: 50
#      zp:  # override the regular photometric calibration parameters
#        cross_match_catalog: gaia_dr3
#        max_catalog_mag: [22.0]
#        mag_range_catalog: 6.0
#        min_catalog_stars: 50

  coaddition:  # override the coaddition parameters in the general "coaddition" config
    coaddition:  # override the way coadds are made, from the general "coaddition" config
      method: swarp
    extraction:  # override the coaddition/regular pipeline config, when extracting for the coadd images
      sources: # override the regular source and psf extraction parameters and the coadd extraction parameters
        measure_psf: true
#        threshold: 3.0
#        method: sextractor
#      bg: # override the regular background estimation parameters and the coadd extraction parameters
#        format: map
#        method: sep
#        poly_order: 1
#        sep_box_size: 128
#        sep_filt_size: 3
#      wcs:  # override the regular astrometric calibration parameters and the coadd extraction parameters
#        cross_match_catalog: gaia_dr3
#        solution_method: scamp
#        max_catalog_mag: [22.0]
#        mag_range_catalog: 6.0
#        min_catalog_stars: 50
#      zp:  # override the regular photometric calibration parameters and the coadd extraction parameters
#        cross_match_catalog: gaia_dr3
#        max_catalog_mag: [22.0]
#        mag_range_catalog: 6.0
#        min_catalog_stars: 50


# ======================================================================
# ======================================================================
# ======================================================================-
# Instrument-specific calibration

# Specific configuration for specific instruments.
# Instruments should override the two defaults from
# instrument_default; they may add additional
# configuration that their code needs.

instrument_default:
  calibratorset: nightly
  flattype: sky


# DECam

# For the observatory-supplied calibrations, NOIRLab distributes them on
# a Google Drive, which is a nightmare to try to download
# programmatically.  What's more, they come in ginormous tar files,
# which are slow to process; especially for tests, where we just want to
# do a couple of chips, it's not worth the time.  So, I've untarred the
# files on the NERSC web portal to allow them to be grabbed
# individually.  These can be found on Perlmutter at
# /global/cfs/dirs/m2218/www/decam_calibration_files

DECam:
  calibratorset: externally_supplied
  flattype: externally_supplied
  calibfiles:
    mjd: 56876
    urlbase: https://portal.nersc.gov/cfs/m4616/decam_calibration_files/
    linearity: DECamMasterCal_56475/linearity/linearity_table_v0.4.fits
    fringebase: DECamMasterCal_56876/fringecor/DECam_Master_20131115v1
    flatbase: DECam_domeflat/
    illuminationbase: DECamMasterCal_56876/starflat/DECam_Master_20130829v3
    bpmbase: DECamMasterCal_56876/bpm/DECam_Master_20140209v2_cd_

# ======================================================================
# ======================================================================
# ======================================================================
# Astromatic utility config (sextractor, scamp, swarp, psfex)
#
# config_dir: string or null
#    An absolute path to the base of the path where astromatic config
#    files are.  If null, will use $CODE_ROOT.
#
# config_subdir: string or null
#   A path relative to config_dir where the astromatic config files are.
#   Can't be null.

astromatic:
  config_dir: null
  config_subdir: share/astromatic
