# ======================================================================
# Customizing the config -- READ THIS
#
# * Do not edit this file, so that it will stay synced with what's in the
#   git archive.
#
# So how do you set up your own local config for your pipeline?  Before
# doing that, it's worth understanding how the config works, as you're
# more likely to do it right.  If you don't want to understand, but just
# want to get on with setting your config (and potentially breaking
# things if you try to do anything complicated), skip the next section
# and search for CUSTOMIZING YOUR CONFIG below.
#
#
# HOW THE CONFIG WORKS
#
# When the pipeline runs, it reads the config file in the environment
# variable SEECHANGE_CONFIG, which by default is this file.  This and
# any other included config file is a yaml file, but it can't be just
# any yaml file; the top level must be a dictionary.
#
# A config file has three special options (only two of which actually
# show up in this file you're reading right now); each of those is a
# list of file paths relative to the directory where this file is found.
#
#   The "current working config" starts as an empty dictionary ({}).  When
#      everything is done, it can be a big messy hierarchy.  Each key of
#      the top level dictionary can have a value that is a scalar, a
#      list, or a dictionary.  The structure is recursive; each element
#      of each list can be a scalar, a list, or a dictionary, and the
#      value associated with each key in each dictionary can itself be a
#      scalar, a list, or a dictionary.
#
#   preloads is a list of files that are read, in order, *before* the
#      current config file, populating the current working config.
#      Files later in the list override files earlier in the list.
#
#   The current file is parsed next.  It overrides anything in the
#      current working config (which will have been built from preload, if
#      any).
#
#   augments is a list of files that are read next.  Those files are
#      read in order, and augment the current working config.
#
#   overrides is a list off files that are read last.  Those files are
#      read in order, and override the current working config.
#
# Any file that's read can itself have preloads, augments, and
# overrides.  IMPORTANT: there is no circular inclusion detection, so if
# you say file A augments file B and file B augments file A, you will
# get stuck in an infinite recursion when you try to read the config.
# This will either lead to a python crash because it detects too many
# recursion levels, or to a process crash if you run out of memory, or
# to the end of the local Universe as you create a singularity and
# nucleate a new Big Bang.  Just don't do that.  Keep it simple.
#
# Above, the words "augment" and "override" were used to describe how to
# combine information from two different files.  Exactly what happens is
# complicated; if you *really* want to know, see
#
#   util/config.py::Config._merge_trees()
#
# Here's an attempt to define it:
#
#   augment
#      When one config tree (the "right" tree) "augments" another (the
#      "left" tree), generally speaking, stuff in the right tree is
#      added to stuff in the left tree, but of course it's more
#      complicated than that.  The augment process walks down the trees,
#      from top to bottom.  It starts at the very top level, where the
#      left dictionary and right dictionary are compared.
#
#         * If the current item being compared have different types
#           (scalar vs. list vs. dict), the new value *replaces* the old
#           values.  This will never happen at the very top level,
#           because both left and right are dictionaries at the top
#           level.
#
#         * If the item being compared is a dictionary, then merge the
#           two dictionaries.  Keys in the right that don't show up in
#           the left key have their key:value added wholesale to
#           the left item.  If a key shows up in both the left and
#           right trees, then recurse.  (So, the value of right[key]
#           augments the value of left[key].)
#
#         * If the item being compared is a list, then then the
#           right list extends the left list.  (Literally using
#           list.extend().)
#
#         * If the item being compared is a scalar, then the right
#           value replaces the left value.
#
#   override
#      When one config tree (the "right" tree) "overrides" another (the
#      "left" tree), generally speaking, stuff in the right tree
#      replaces stuff in the left tree.  The override process walks down
#      the trees, from top ot bottom.  It starts at the very top level,
#      where the left dictionary and the right dictionary are compared.
#
#         * If the current item being compared have different types
#           (scalar vs. list vs. dict), the new value *replaces* the old
#           values.  This will never happen at the very top level,
#           because both left and right are dictionaries at the top
#           level.  This is exactly the same behavior as in augment.
#
#         * If the current item being compared is a dictionary, then
#           the dictionaries are merged in exactly the same manner
#           as "augment", with the modification that recursing down
#           into the dictionary passes along the fact that we're
#           overriding rather than augmenting.
#
#         * If the current item being compared is a list, then the
#           right list *replaces* the left list.  (This could
#           potentially throw away a gigantic hierarchy if lists
#           and dicts and scalars from the left wide, which is as
#           designed.)
#
#         * If the item being compared is a scalar, then the right value
#
# This can be very confusing, so keeping your config overrides and
# augments simple is likely what you want.
#
#
# CUSTOMIZING YOUR CONFIG
#
# As noted above, do not edit this file.  There are two ways you can
# proceed:
#
# (1) Create your own new config file.  Point the SEECHANGE_CONFIG env
#     var at that config file.
#
# (2) Use this config file as is.  Edit the files "local_overrides.yaml"
#     and "local_augments.yaml" to put in any modifications to your
#     config you want relative to this file.  Those two files are
#     excluded from the git archive, so your changes won't be saved
#     if you push, and won't be overwritten if you pull.  (If those two
#     files aren't empty, then the tests may fail, but if you're doing
#     this, you're probably setting up a working tree, not setting up
#     tests, so you probably don't care.)
#
#     How do you know which one to use?  Either read the long section
#     above, or just use "local_overrides.yaml" and put in any config
#     options that you want changed, and then read everything above
#     if it does not behave as you expected.
#
#
# WARNING : python's yaml reading is broken.  It will read 1e10 as a
# string, not a float.  Write 1.0e+10 to make it work right.  There are
# two things here: the first is the + after e (which, I think *is* part
# of the yaml spec, even though we can freely omit that + in Python and
# C).  The second is the decimal point; the YAML spec says it's not
# necessary, but python won't recognize it as a float without it.

overrides:
  - local_overrides.yaml
augments:
  - local_augments.yaml

# ======================================================================
# Config Chooser
#
# OMG.  Everything you read above about editing the config?  It's about
# to get more complicated.  The first pipeline process that generally
# runs is the ConfigChooser process.  That process may decide, based on
# the configuration below, to replace the default config file with
# a different one.  However, if you look at those different onces,
# you will notice that they have this file as a preload, so this
# file still largely applies.

configchoice:
  config_dir: null
  gaia_density_catalog: share/gaia_density/gaia_healpix_density.pq
  choice_algorithm: star_density
  star_mag_cutoff: 20
  star_density_cutoff: 1e5    # TODO : better determine this value  (1e5 is at least vaugely plausible)
  configs:
    galactic: default_config_galactic.yaml
    extragalactic: default_config_extragalactic.yaml

# ======================================================================
# Paths

path:
  data_root: null
  data_temp: null

# ======================================================================
# Databse config

db:
  engine: postgresql
  user: postgres
  password: fragile
  password_file: null
  host: localhost
  port: 5432
  database: seechange

# ======================================================================
# Local file storage
#
# format: string
#   Format for image files.  Currently only "fits" and "fitsfz" are
#   supported.  We have vague ambitions to support HDF5 files.  ASDF has
#   only very vaguely been considered by one person.
#
# single_file: bool
#   If True, then weight, flags will be stored in the same file with the
#   image data (as different HDU extensions for FITS files).  If False,
#   then image, weight, flags are three separate FITS files.
#   single_file=True is not really fully supported at the moment, and
#   would need some development and a bunch of thought before one should
#   trust using it.
#
# name_convention: string
#   Use any of the following: inst_name, date, time, section_id, filter,
#   ra, dec, prov_id.  Can also use section_id_int if the section_id is
#   always an integer.  Can also use ra_int and ra_frac to get the integer
#   number before/after the decimal point (the same can be done for
#   dec).  Also use ra_int_h to get the number in hours, etc.  To get the
#   declination with "p" or "m" replacing the sign, use dec_int_pm.  The
#   string given here is fed into the python format() function so you
#   can use e.g., {ra_int:03d} to get a 3 digit zero padded right
#   ascension.  The name convention can also include subfolders (e.g.,
#   using {ra_int}/...).  The minimal set of fields to make the
#   filenames unique include: inst_name (short instrument name), date,
#   time, section_id, prov_hash (in this example, the first six
#   characters of the provenance unique hash)

storage:
  images:
    format: fits
    single_file: false
    name_convention: "{ra_int:03d}/{inst_name}_{date}_{time}_{section_id}_{filter}_{im_type}_{prov_hash:.6s}"


# ======================================================================
# Archive:
#
# Set to null if there is no archive; otherwise, a dict
#
# Fields of the dict:
#
#   archive_url: string or null
#     The URL of the archive server, or null if archive is on the filesystem
#
#   verify_cert: bool
#     Should we verify the SSL cert of the archive server?  You usually
#     want this to be true, but at least for testing we set this to
#     false so we don't have to deal with getting certificates for
#     our internally contained docker environment.
#
#   path_base: string
#     The base of the collection on the archive server (a string unique
#     to this dataset).  (This is not a full path, but relative to the
#     base of the overall archive, and will usually just be a single
#     word.)
#
#   local_read_dir: string or null
#     The directory to read from if the archive is on the local
#     filesystem, or null.  Different from local_write_dir in case
#     the system has a faster read-only method of accessing the same
#     files (which is the case on NERSC).  If null, then the archive
#     is not accessible locally, and the archive_url server will be
#     used instead.  May be the same as local_write_dir.
#
#   local_write_dir: string or null
#     The directory to write to if the archive is on the local
#     filesystem, or null if we need to use archive_url.

archive: null


# ======================================================================
# Conductor
#
# The conductor is the service (running on the seechange webap) that the
# pipeline uses to figure out what exposures are available and need
# processing.
#
# (There will be other values set in a config file used by the actual
# conductor.)
#
#   conductor_url: string
#     The default value here is what we use in our test environment.
#     It's almost certainly wrong for any production config.
#
#   username: string
#     The username for authentication on the conductor.
#
#   password: string
#     The password for authentication on the conductor.
#
#   password_file: string
#     A file containing the password for authentication on the
#     conductor.  Set password to null and then set this if
#     you have a secure "secrets" directory where you keep passwords,
#     so that you don't have to put the password in an actual
#     config file that might be publicly readable or that might
#     get committed to a git archive.

conductor:
  conductor_url: https://webap:8081/
  username: unknown
  password: null
  password_file: null

# ======================================================================
# Webap
#
# This config is not used by the main pipeline, only the webap.
# When building the Dockerfile, those configs will be set from
# webap/seechange_webap.yaml and webap/local_overrides.yaml
# (though they include this file)

webap:
  webap_url: https://webap:8081/
  flask_secret_key: null
  flask_secret_key_file: null

# ======================================================================
# Email for Webap
#
# These configs are only used by the webap; their values do not matter
# for the main pipeline.  However, they must be set properly in the
# config files used when building the Docker image for the conductor and
# the webap.  The configs below are what's needed for our test
# environment, and are definitely wrong for any production environment.

email:
  email_from: 'Seechange <nobody@nowhere.org>'
  email_subject: 'Seechange password reset'
  email_system_name: 'Seechange'
  smtp_server: 'mailhog'
  smtp_port: 1025
  smtp_use_ssl: false
  smtp_username: null
  smtp_password: null
  smtp_password_file: null


# ======================================================================
# Gaia DR3 server
#
# Gaia stars are used for both astrometric and photometric
# calibration.  (As this is a discovery pipeline, we don't
# promise photometric calibration to better than a few %.)
#
# There are two ways we can get it : through the server defined in the
#  submodule extern/nersc-desi-gaia-dr3-server, and via NOIRLab
#  using their queryClient.
#
#   use_server: bool
#      If true, use the custom gaia server.
#
#   server_url: string
#      The url of the custom server
#
#   fallback_datalab: bool
#      If use_server is False, or if user_server is True and the custom
#      server is unavailable or fails after five tries, then try falling
#      back to the NOIRLab server if fallback_datalab is True.

catalog_gaiadr3:
  use_server: True
  fallback_datalab: False
  server_url: https://ls4-gaia-dr3.lbl.gov
  server_timeout_sec: 5.


# ======================================================================
# ======================================================================
# ======================================================================
# Pipeline components
#

# ======================================================================
# Overall pipeline
#
# For additional documentation on the parameters, see the Parameters
# subclass in the file that defines each part of the pipeline
#
# save_before_subtraction : bool
#    If true, save images and data products after
#    initial reduction but before the pipeline looks for a reference
#    to make a subtraction
#
# save_at_finish : bool
#    Save everything at the end of the pipeline run.
#
# provenance_tag: string
#    The ProvenanceTag that products of the pipeline should be
#    associated with
#

pipeline:
  save_before_subtraction: true
  save_at_finish: true
  provenance_tag: current
  inject_fakes: false

# ======================================================================
# Preprocessing
#
# steps_required : list of str
#    Steps that must be done on the image.  Either they come with all
#    these steps done, or we must perform them.  (If they come with all
#    steps done, the ideally the pipeline will flag them as such when
#    ingesting them and won't incorrectly do something twice.)

preprocessing:
  steps_required: [ 'overscan', 'linearity', 'flat', 'fringe' ]

# ======================================================================
# Extraction and Backgrounding are done in one step.
#
# (Reason: you might want to find sources to mask them for finding
#  the background.  Or, you might want to subtract the background
#  before finding sources.  Or you might want to iterate back and
#  forth.)
#
extraction:
  method: sextractor
  measure_psf: true
  apertures: [1.0, 2.0, 3.0, 5.0]
  inf_aper_num: -1
  best_aper_num: 0
  aperunit: fwhm
  separation_fwhm: 1.0
  threshold: 3.0
  subtraction: false
  backgrounding:
    format: map
    method: sep
    poly_order: 1
    box_size: null
    filt_size: null

# ======================================================================
# World Coordinates

astrocal:
  cross_match_catalog: gaia_dr3
  solution_method: scamp
  max_catalog_mag: [22.0]
  mag_range_catalog: 6.0
  min_catalog_stars: 50
  max_sources_to_use: [2000, 1000, 500, 200]

# ======================================================================
# Zero Point

photocal:
  cross_match_catalog: gaia_dr3
  max_catalog_mag: [22.0]
  mag_range_catalog: 6.0
  min_catalog_stars: 50

# ======================================================================
# Subtraction
#
# (The pipeline will also do reference finding, but the config for
# that is further down.)
#
# refset:
#    "set refset to null to only make references (no subtraction will
#     happen to start running subtractions, first make a ref set and use
#     the name of than. in this field." <-- TODO look a this comment.
#     It seems odd.  Look at the code, and see if this is really the
#     behavior.  It seems like weird behavior.  Better would be to add a
#     "steptotodo" parameter to the overall pipeline above.  Reference
#     generation should have its own executable, not be a side effect of
#     non-obvious parameter settings in the subtraction/discvoery
#     pipeline.

subtraction:
  method: zogy
  refset: null
  alignment_index: new
  alignment:
    method: swarp
  reference:
    minovfrac: 0.85
    must_match_instrument: true
    must_match_filter: true
    must_match_section: false
    must_match_target: false

# ======================================================================#
# Detection
#
# Finding sources on difference images
#
# subtraction : bool
#    This should always be true, because in the nomenclature of this
#    pipeline, "detection" is what we run on a difference images, and
#    "extraction" is what we run on science images, even though the
#    actual code run in both cases may well be the same.
#
# method : str
#    For the ZOGY subtraction method, the detection method must be
#    filter.  For the hotpants method, the detection method must be
#    sextractor.
#
# See pipline/subtraction.py for parameter definitions

detection:
  subtraction: true
  method: filter
  threshold: 5.0

# ======================================================================#
# Cutting
#
# Making new, ref, sub thumbnail cutouts.
#
# The RBbot models we're using have a predetermined cutout size, which
# must match what's done here.  The models we're using now require a
# 41×41 cutout.

cutting:
  cutout_size: 41

# ======================================================================
# Measuring
#
# Measuring fluxes and positions of detections, as well as making
# analytical cuts.
#
# If you want (for testing purposes) to have all measurements
# saved regardless of whether they're good or not, set all
# the deletion thresholds to null.  (A few measurements will
# still get filtered out, if most of the pixels in the region
# of the source is masked.)
#
# See pipeline/measuring.py for definition of the parameters

measuring:
  annulus_radii: [ 4., 5. ]
  annulus_units: fwhm
  use_annulus_bg_on_sub: False
  diag_box_halfsize: 2.
  diag_box_halfsize_unit: fwhm
  negatives_n_sigma_outlier: 2.
  bad_thresholds:
    psf_fit_flags_bitmask: 0x2e
    detection_dist: 5.
    gaussfit_dist: 5.
    elongation: 3.
    width_ratio: 2.
    nbadpix: 1
    negfrac: 0.3
    negfluxfrac: 0.3
  deletion_thresholds:
    psf_fit_flags_bitmask: 0x2e
    detection_dist: 5.
    gaussfit_dist: 5.
    elongation: 3.
    width_ratio: 2.
    nbadpix: 1
    negfrac: 0.3
    negfluxfrac: 0.3
    # psf_fit_flags_bitmask: null
    # detection_dist: null
    # gaussfit_dist: null
    # elongation: null
    # width_ratio: null
    # nbadpix: null
    # negfrac: null
    # negfluxfrac: null
  association_radius: 2.0
  do_not_associate: False


# ======================================================================
# Scoring
#
# This is for "deep scoring", i.e. (probably) deep learning methods for
# assigning an empirical score to the cutouts.

scoring:
  algorithm: RBbot-quiet-shadow-131-cut0.55
  rbbot_model_dir: /seechange/share/RBbot_models

# ======================================================================
# Object configuration
#
# namefmt = format string for names

object:
  namefmt: obj%Y%a

# ======================================================================
# Alert generation
#
# Set "send_alerts" to false to completely disable alert generation.
#
# Alerts may go to multiple destinations, so the configuration includes
# a list of dicts with parameters.  Those parameters include at least
# 'method' and 'enabled'; method determines the actual code used to send
# the alert.  enabled indicates whether this method is currently
# used. deepcut is the scoring threshold (0-1) above which a detection
# will be sent out as an alert.  Further parameters are method dependent.
# The same method may show up more than once, e.g. if alerts are being
# sent to multiple kafka servers, or if alerts are sent to multiple
# different topics on the same kafka server with different parameters.
#
# 'deepcut': if Null, use the nominal value for the algorithm
# (in DeepScore.get_rb_cut).  If a float, use that.  This is the r/b
# cut below which no alert will be generated.
#
# kafka_topic_pattern : TODO DOCS

alerts:
  send_alerts: true
  methods:
    - name: filtered_kafka_stream
      method: kafka
      enabled: false
      deepcut: null
      avro_schema: /seechange/share/avsc/alert.avsc
      kafka_server: kafka.server.com
      kafka_topic_pattern: "topic_{year:04d}-{month:02d}-{day:02d}"
      previous_source_days: 365
    - name: junk_included_kafka_stream
      method: kafka
      enabled: false
      deepcut: 0.
      avro_schema: /seechange/share/avsc/alert.avsc
      kafka_server: kafka.server.com
      kafka_topic_pattern: "topic_with_junk_{year:04d}-{month:02d}-{day:02d}"
      previous_source_days: 365

# ======================================================================
# Fake injection

fakeinjection:
  min_fake_mag: -2.
  max_fake_mag: 1.
  mag_rel_limmag: true
  num_fakes: 100
  mag_prob_ratio: 1.
  random_seed: 0
  hostless_frac: 1.
  host_minmag: -3.
  host_maxmag: 0.5
  host_distscale: 1.


# ======================================================================
# ======================================================================
# ======================================================================
# Coaddition pipeline
#
# pipeline:
#   date_range: number of days before end date to set the start date
#               when searching for images to caodd.  (TODO : is this used???)
#
# coaddition: see pipeline/coaddition.py
#
# extraction: These parameters override the extraction parameters
#             set above.  If nothing is here, then that
#             config will be used as is.


coaddition:
  pipeline:
    date_range: 7.0
  coaddition:
    method: zogy
    noise_estimator: sep
    flag_fwhm_factor: 1.0
    alignment_index: last
    alignment:
      method: swarp
    inpainting:
      multi_image_method: median
      feather_width: 2
      rescale_method: median
      single_image_method: biharmonic
      ignore_flags: 0
  extraction:
    measure_psf: true
    threshold: 3.0
    method: sextractor
  backgrounding:
    format: map
    method: sep
  astrocal:
    cross_match_catalog: gaia_dr3
    solution_method: scamp
    max_catalog_mag: [22.0]
    mag_range_catalog: 6.0
    min_catalog_stars: 50
  photocal:
    cross_match_catalog: gaia_dr3
    max_catalog_mag: [22.0]
    mag_range_catalog: 6.0
    min_catalog_stars: 50

# ======================================================================
# ======================================================================
# ======================================================================
# Reference making


# use these parameters to make references by coadding images
# the reference pipeline will load the regular configuration first,
# then the coaddition config, and only in the end, override with this:
referencing:
  maker:  # these handles the top level configuration (e.g., how to choose images that go into the reference maker)
    name: best_references  # the name of the reference set that you want to make references for
    description: "The Best References"   # Description of the reference set
    start_time: null  # only grab images after this time
    end_time: null  # only grab images before this time
    instrument: null  # only grab images from this instrument (if list, will make cross-instrument references)
    # filter: null  # only grab images with this filter
    project: null  # only grab images with this project
    min_airmass: null  # only grab images with airmass above this
    max_airmass: null  # only grab images with airmass below this
    min_background: null  # only grab images with background rms above this
    max_background: null  # only grab images with background rms below this
    min_seeing: null  # only grab images with seeing above this
    max_seeing: null  # only grab images with seeing below this
    min_lim_mag: null  # only grab images with limiting magnitude above this
    max_lim_mag: null  # only grab images with limiting magnitude below this
    min_exp_time: null  # only grab images with exposure time above this
    max_exp_time: null  # only grab images with exposure time below this
    min_number: 7  # only create a reference if this many images can be found
    max_number: 30  # do not add more than this number of images to the reference
    corner_distance: 0.8  # see the docs
    overlap_fraction: 0.9
    coadd_overlap_fraction: 0.1
    min_only_center: False
    seeing_quality_factor: 3.0  # linear coefficient for adding lim_mag and seeing to get the "image quality"
    save_new_refs: true  # should the new references be saved to disk and committed to the database?

  coaddition:  # override the coaddition parameters in the general "coaddition" config
    coaddition:  # override the way coadds are made, from the general "coaddition" config
      method: swarp
    extraction:  # override the coaddition/regular pipeline config, when extracting for the coadd images
      measure_psf: true
#      threshold: 3.0
#      method: sextractor
#    backgrounding: # override the regular background estimation parameters and the coadd extraction parameters
#      format: map
#      method: sep
#      poly_order: 1
#      sep_box_size: 128
#      sep_filt_size: 3
#    astrocal:  # override the regular astrometric calibration parameters and the coadd extraction parameters
#      cross_match_catalog: gaia_dr3
#      solution_method: scamp
#      max_catalog_mag: [22.0]
#      mag_range_catalog: 6.0
#      min_catalog_stars: 50
#    photocal:  # override the regular photometric calibration parameters and the coadd extraction parameters
#      cross_match_catalog: gaia_dr3
#      max_catalog_mag: [22.0]
#      mag_range_catalog: 6.0
#      min_catalog_stars: 50


# ======================================================================
# ======================================================================
# ======================================================================-
# Instrument-specific calibration

# Specific configuration for specific instruments.
# Instruments should override the two defaults from
# instrument_default; they may add additional
# configuration that their code needs.

instrument_default:
  calibratorset: nightly
  flattype: sky


# DECam

# For the observatory-supplied calibrations, NOIRLab distributes them on
# a Google Drive, which is a nightmare to try to download
# programmatically.  What's more, they come in ginormous tar files,
# which are slow to process; especially for tests, where we just want to
# do a couple of chips, it's not worth the time.  So, I've untarred the
# files on the NERSC web portal to allow them to be grabbed
# individually.  These can be found on Perlmutter at
# /global/cfs/dirs/m4616/www/decam_calibration_files

DECam:
  calibratorset: externally_supplied
  flattype: externally_supplied
  calibfiles:
    mjd: 56876
    urlbase: https://portal.nersc.gov/cfs/m4616/decam_calibration_files/
    linearity: DECamMasterCal_56475/linearity/linearity_table_v0.4.fits
    fringebase: DECamMasterCal_56876/fringecor/DECam_Master_20131115v1
    flatbase: DECam_domeflat/
    illuminationbase: DECamMasterCal_56876/starflat/DECam_Master_20130829v3
    bpmbase: DECamMasterCal_56876/bpm/DECam_Master_20140209v2_cd_

# ======================================================================
# ======================================================================
# ======================================================================
# Astromatic utility config (sextractor, scamp, swarp, psfex)
#
# config_dir: string or null
#    An absolute path to the base of the path where astromatic config
#    files are.  If null, will use $CODE_ROOT.
#
# config_subdir: string or null
#   A path relative to config_dir where the astromatic config files are.
#   Can't be null.

astromatic:
  config_dir: null
  config_subdir: share/astromatic
