import os
import base64
import hashlib
import itertools

import numpy as np

import sqlalchemy as sa
from sqlalchemy import orm

from sqlalchemy.dialects.postgresql import JSONB
from sqlalchemy.dialects.postgresql import UUID as sqlUUID
from sqlalchemy.ext.declarative import declared_attr
from sqlalchemy.ext.hybrid import hybrid_property
from sqlalchemy.orm.exc import DetachedInstanceError
from sqlalchemy.schema import CheckConstraint

from astropy.time import Time
from astropy.wcs import WCS
from astropy.io import fits
import astropy.coordinates
import astropy.units as u

from util.util import read_fits_image, save_fits_image_file, parse_dateobs, listify, asUUID
from util.radec import parse_ra_hms_to_deg, parse_dec_dms_to_deg
from util.logger import SCLogger

from models.base import (
    Base,
    SeeChangeBase,
    SmartSession,
    UUIDMixin,
    FileOnDiskMixin,
    SpatiallyIndexed,
    FourCorners,
    HasBitFlagBadness,
)
from models.provenance import Provenance
from models.exposure import Exposure
from models.instrument import get_instrument_instance
from models.enums_and_bitflags import (
    ImageFormatConverter,
    ImageTypeConverter,
    string_to_bitflag,
    bitflag_to_string,
    image_preprocessing_dict,
    image_preprocessing_inverse,
    image_badness_inverse,
)

import util.config as config

from improc.tools import sigma_clipping


# links many-to-many Image to all the Images used to create it
image_upstreams_association_table = sa.Table(
    'image_upstreams_association',
    Base.metadata,
    sa.Column('upstream_id',
              sqlUUID,
              sa.ForeignKey('images._id', ondelete="RESTRICT", name='image_upstreams_association_upstream_id_fkey'),
              primary_key=True),
    sa.Column('downstream_id',
              sqlUUID,
              sa.ForeignKey('images._id', ondelete="CASCADE", name='image_upstreams_association_downstream_id_fkey'),
              primary_key=True),
)


class Image(Base, UUIDMixin, FileOnDiskMixin, SpatiallyIndexed, FourCorners, HasBitFlagBadness):

    __tablename__ = 'images'

    @declared_attr
    def __table_args__( cls ):
        return (
            CheckConstraint( sqltext='NOT(md5sum IS NULL AND '
                               '(md5sum_extensions IS NULL OR array_position(md5sum_extensions, NULL) IS NOT NULL))',
                               name=f'{cls.__tablename__}_md5sum_check' ),
            sa.Index(f"{cls.__tablename__}_q3c_ang2ipix_idx", sa.func.q3c_ang2ipix(cls.ra, cls.dec)),
        )

    _format = sa.Column(
        sa.SMALLINT,
        nullable=False,
        default=ImageFormatConverter.convert('fits'),
        doc="Format of the file on disk. Should be fits or hdf5. "
    )

    @hybrid_property
    def format(self):
        return ImageFormatConverter.convert(self._format)

    @format.inplace.expression
    @classmethod
    def format(cls):
        # ref: https://stackoverflow.com/a/25272425
        return sa.case(ImageFormatConverter.dict, value=cls._format)

    @format.inplace.setter
    def format(self, value):
        self._format = ImageFormatConverter.convert(value)

    exposure_id = sa.Column(
        sa.ForeignKey('exposures._id', ondelete='SET NULL', name='images_exposure_id_fkey'),
        nullable=True,
        index=True,
        doc=(
            "ID of the exposure from which this image was derived. "
            "Only set for single-image objects."
        )
    )

    ref_image_id = sa.Column(
        sa.ForeignKey('images._id', ondelete="SET NULL", name='images_ref_image_id_fkey'),
        nullable=True,
        index=True,
        doc="ID of the reference image used to produce this image, in the upstream_images list. "
    )

    @property
    def new_image_id(self):
        """Get the id of the image that is NOT the reference image. Only for subtractions (with ref+new upstreams)"""
        # TODO : this will return something if it's a coadd of two images.
        # Perhaps we should check self.is_sub, and return None if that's false?
        image = [ i for i in self.upstream_image_ids if i != self.ref_image_id ]
        if len(image) == 0 or len(image) > 1:
            return None
        return image[0]

    @property
    def upstream_image_ids( self ):
        if self._upstream_ids is None:
            with SmartSession() as session:
                them = list ( session.query( image_upstreams_association_table.c.upstream_id,
                                             Image.mjd )
                              .join( Image, Image._id == image_upstreams_association_table.c.upstream_id )
                              .filter( image_upstreams_association_table.c.downstream_id == self.id )
                              .all() )
            them.sort( key=lambda x: x[1] )
            self._upstream_ids = [ t[0] for t in them ]
        return self._upstream_ids

    @upstream_image_ids.setter
    def upstream_image_ids( self, val ):
        raise RuntimeError( "upstream_ids cannot be set directly.  Set it by creating the image with "
                            "from_images() or from_ref_and_new()" )

    is_sub = sa.Column(
        sa.Boolean,
        nullable=False,
        default=False,
        index=True,
        doc='Is this a subtraction image.'
    )

    is_coadd = sa.Column(
        sa.Boolean,
        nullable=False,
        default=False,
        index=True,
        doc='Is this image made by stacking multiple images.'
    )

    _type = sa.Column(
        sa.SMALLINT,
        nullable=False,
        default=ImageTypeConverter.convert('Sci'),
        index=True,
        doc=(
            "Type of image. One of: [Sci, Diff, Bias, Dark, DomeFlat, SkyFlat, TwiFlat, Warped] "
            "or any of the above types prepended with 'Com' for combined "
            "(e.g., a ComSci image is a science image combined from multiple exposures)."
            "Saved as an integer in the database, but converted to a string when read. "
        )
    )

    @hybrid_property
    def type(self):
        return ImageTypeConverter.convert(self._type)

    @type.inplace.expression
    @classmethod
    def type(cls):
        return sa.case(ImageTypeConverter.dict, value=cls._type)

    @type.inplace.setter
    def type(self, value):
        self._type = ImageTypeConverter.convert(value)

    provenance_id = sa.Column(
        sa.ForeignKey('provenances._id', ondelete="CASCADE", name='images_provenance_id_fkey'),
        nullable=False,
        index=True,
        doc=(
            "ID of the provenance of this image. "
            "The provenance will contain a record of the code version "
            "and the parameters used to produce this image. "
        )
    )

    info = sa.Column(
        JSONB,
        nullable=False,
        default={},
        doc=(
            "Additional information on the this image. "
            "Only keep a subset of the header keywords, "
            "and re-key them to be more consistent. "
        )
    )

    mjd = sa.Column(
        sa.Double,
        nullable=False,
        index=True,
        doc=(
            "Modified Julian date of the exposure (MJD=JD-2400000.5). "
            "Multi-exposure images will have the MJD of the first exposure."
        )
    )

    @property
    def observation_time(self):
        """Translation of the MJD column to datetime object."""
        if self.mjd is None:
            return None
        else:
            return Time(self.mjd, format='mjd').datetime

    @property
    def start_mjd(self):
        """Time of the beginning of the exposure, or set of exposures (equal to mjd). """
        return self.mjd

    @property
    def mid_mjd(self):
        """Time of the middle of the exposures.
        For multiple, coadded exposures (e.g., references), this would
        be the middle between the start_mjd and end_mjd, regarless of
        how the exposures are spaced.
        """
        if self.start_mjd is None or self.end_mjd is None:
            return None
        return (self.start_mjd + self.end_mjd) / 2.0

    end_mjd = sa.Column(
        sa.Double,
        nullable=False,
        index=True,
        doc=(
            "Modified Julian date of the end of the exposure. "
            "Multi-image object will have the end_mjd of the last exposure."
        )
    )

    exp_time = sa.Column(
        sa.REAL,
        nullable=False,
        index=True,
        doc="Exposure time in seconds. Multi-exposure images will have the total exposure time."
    )

    instrument = sa.Column(
        sa.Text,
        nullable=False,
        index=True,
        doc="Name of the instrument used to create this image. "
    )

    telescope = sa.Column(
        sa.Text,
        nullable=False,
        index=True,
        doc="Name of the telescope used to create this image. "
    )

    filter = sa.Column(sa.Text, nullable=True, index=True, doc="Name of the filter used to make this image. ")

    section_id = sa.Column(
        sa.Text,
        nullable=True,
        index=True,
        doc='Section ID of the image, possibly inside a larger mosiaced exposure. '
    )

    project = sa.Column(
        sa.Text,
        nullable=False,
        index=True,
        doc='Name of the project (could also be a proposal ID). '
    )

    target = sa.Column(
        sa.Text,
        nullable=False,
        index=True,
        doc='Name of the target object or field id. '
    )

    preproc_bitflag = sa.Column(
        sa.SMALLINT,
        nullable=False,
        default=0,
        index=False,
        doc='Bitflag specifying which preprocessing steps have been completed for the image.'
    )

    @property
    def preprocessing_done(self):
        """Return a list of the names of preprocessing steps that have been completed for this image."""
        return bitflag_to_string(self.preproc_bitflag, image_preprocessing_dict)

    @preprocessing_done.setter
    def preprocessing_done(self, value):
        self.preproc_bitflag = string_to_bitflag(value, image_preprocessing_inverse)

    astro_cal_done = sa.Column(
        sa.BOOLEAN,
        nullable=False,
        default=False,
        index=False,
        doc=( 'Has a WCS been solved for this image.  This should be set to true after astro_cal '
              'has been run, or for images (like subtractions) that are derived from other images '
              'with complete WCSes that can be copied.  This does not promise that the "latest and '
              'greatest" astrometric calibration is what is in the image header, only that there is '
              'one from the pipeline that should be good for visual identification of positions.' )
    )

    sky_sub_done = sa.Column(
        sa.BOOLEAN,
        nullable=False,
        default=False,
        index=False,
        doc='Has the sky been subtracted from this image. '
    )

    airmass = sa.Column(
        sa.REAL,
        nullable=True,
        index=True,
        doc='Airmass of the observation. '
    )

    fwhm_estimate = sa.Column(
        sa.REAL,
        nullable=True,
        index=True,
        doc=(
            'FWHM estimate for the image, in arcseconds, '
            'from the first time the image was processed.'
        )
    )

    zero_point_estimate = sa.Column(
        sa.REAL,
        nullable=True,
        index=True,
        doc=(
            'Zero point estimate for the image, '
            'from the first time the image was processed.'
        )
    )

    lim_mag_estimate = sa.Column(
        sa.REAL,
        nullable=True,
        index=True,
        doc=(
            'Limiting magnitude estimate for the image, '
            'from the first time the image was processed.'
        )
    )

    bkg_mean_estimate = sa.Column(
        sa.REAL,
        nullable=True,
        index=True,
        doc=(
            'Background estimate for the image, '
            'from the first time the image was processed.'
        )
    )

    bkg_rms_estimate = sa.Column(
        sa.REAL,
        nullable=True,
        index=True,
        doc=(
            'Background RMS residual (i.e. sky noise ) '
            'estimate for the image, from the first time '
            'the image was processed.'
        )
    )


    def _get_inverse_badness(self):
        """Get a dict with the allowed values of badness that can be assigned to this object"""
        return image_badness_inverse

    saved_extensions = [
        'data',
        'flags',
        'weight',
        'score',  # the matched-filter score of the image (e.g., from ZOGY)
        'psfflux',  # the PSF-fitted equivalent flux of the image (e.g., from ZOGY)
        'psffluxerr', # the error in the PSF-fitted equivalent flux of the image (e.g., from ZOGY)
    ]

    def __init__(self, *args, **kwargs):
        FileOnDiskMixin.__init__(self, *args, **kwargs)
        HasBitFlagBadness.__init__(self)
        SeeChangeBase.__init__(self)  # don't pass kwargs as they could contain non-column key-values

        self.raw_data = None  # the raw exposure pixels (2D float or uint16 or whatever) not saved to disk!
        self._header = None  # the header data taken directly from the FITS file

        # these properties must be added to the saved_extensions list of the Image
        self._data = None  # the underlying pixel data array (2D float array)
        self._flags = None  # the bit-flag array (2D int array)
        self._weight = None  # the inverse-variance array (2D float array)
        self._score = None  # the image after filtering with the PSF and normalizing to S/N units (2D float array)
        self._psfflux = None  # the PSF-fitted equivalent flux of the image (2D float array)
        self._psffluxerr = None  # the error in the PSF-fitted equivalent flux of the image (2D float array)

        self._nandata = None  # a copy of the image data, only with NaNs at each flagged point. Lazy calculated.
        self._nanscore = None  # a copy of the image score, only with NaNs at each flagged point. Lazy calculated.

        self._upstream_ids = None

        self._instrument_object = None
        self._bitflag = 0
        self.is_sub = False

        if 'header' in kwargs:
            kwargs['_header'] = kwargs.pop('header')

        # manually set all properties (columns or not)
        for key, value in kwargs.items():
            if hasattr(self, key):
                setattr(self, key, value)

        self.calculate_coordinates()  # galactic and ecliptic coordinates

    # def __setattr__(self, key, value):
    #     if key == 'upstream_images':
    #         # make sure the upstream_images list is sorted by mjd:
    #         value.sort(key=lambda x: x.mjd)

    #     super().__setattr__(key, value)

    @orm.reconstructor
    def init_on_load(self):
        SeeChangeBase.init_on_load(self)
        FileOnDiskMixin.init_on_load(self)
        self.raw_data = None
        self._header = None

        for att in self.saved_extensions:
            setattr(self, f'_{att}', None)

        self._nandata = None
        self._nanscore = None

        self._upstream_ids = None

        self._instrument_object = None

        # this_object_session = orm.Session.object_session(self)
        # if this_object_session is not None:  # if just loaded, should usually have a session!
        #     self.load_upstream_products(this_object_session)

    def insert( self, verifyupstreams=False, session=None ):
        """Add the Image object to the database.

        In any events, if there are no exceptions, self.id will be set upon
        return.  (As a side effect, may also load self._upstream_ids, but
        that's transparent to the user, and happens when the user accesses
        upstream_image_ids anyway.)

        This calls UUIDMixin.insert, but also will create assocations
        defined in self._upstreams (which will have been set if this
        Image was created with from_images() or from_ref_and_new()).

        Parameters
        ----------
          verifyupstreams: bool, default False
            If the object was not inserted into the database, verify that
            self._upstream_ids is consistent with the image upstreams
            in the database.  (self._upstream_ids=None is consistent, because
            that means it will just get loaded later.)

          session: SQLAlchemy Session, default None
            Usually you do not want to pass this; it's mostly for other
            upsert etc. methods that cascade to this.

        Returns
        -------
          was_inserted: bool
            True = the image was newly inserted.  False = the object was
            already in the database.

            If the image was in the database and onlyinsert was True,
            or if the id of the object in the database didn't match a
            non-None id of self, an execption will be raised.


        Returns True if the image was newly loaded, False if the image
        was already in the database.

        """

        with SmartSession( session ) as sess:
            try:
                # Lock both the images and image_upstreams_association tables to avoid race
                #   conditions with another process trying to insert the same image.
                self._get_table_lock( sess )
                if ( self._upstream_ids is not None ) and ( len(self._upstream_ids) > 0 ):
                    self._get_table_lock( sess, 'image_upstreams_association' )

                # This will raise an exception if the image is already present; in that case,
                #   the upstream associations should also already be present (loaded by whoever
                #   loaded the image), so it's fine to just error out.
                UUIDMixin.insert( self, session=sess )

                if ( self._upstream_ids is not None ) and ( len(self._upstream_ids) > 0 ):
                    for ui in self._upstream_ids:
                        sess.execute( sa.text( "INSERT INTO "
                                               "image_upstreams_association(upstream_id,downstream_id) "
                                                   "VALUES (:them,:me)" ),
                                      { "them": ui, "me": self.id } )
                    SCLogger.debug( "Image.insert comitting" )
                    sess.commit()

            finally:
                # Make sure the table locks are released
                SCLogger.debug( "Image.insert rolling back" )
                sess.rollback()

    def merge_all(self, session):

        """Merge self and all its downstream products and assign them back to self.

        This includes: sources, psf, wcs, zp.
        This will also merge relationships, such as exposure or upstream_images,
        but that happens automatically using SQLA's magic.

        Must provide a session to merge into. Need to commit at the end.

        Returns the merged image with all its products on the same session.

        DEVELOPER NOTE: changing what gets merged in this function
        requires a corresponding change in
        pipeline/data_store.py::DataStore.save_and_commit

        """
        raise RuntimeError( "merge_all should no longer be necessary; use upsert or insert" )


    def set_corners_from_header_wcs( self, wcs=None, setradec=False ):
        """Update the image's four corners (and, optionally, RA/Dec) from a WCS.

        Parameters
        ----------
        wcs : astropy.wcs.WCS, default None
           The WCS to use.  If None (default), will use the WCS parsed
           from the image header.

        setradec : bool, default False
           If True, also update the image's ra and dec fields, as well
           as the things calculated from it (galactic, ecliptic
           coordinates).

        """
        if wcs is None:
            wcs = WCS( self._header )
        # Try to detect a bad WCS
        if ( wcs.axis_type_names == ['', ''] ):
            raise ValueError( "Could not find a good WCS" )

        ras = []
        decs = []
        # Note: this used to prefer raw_data; changed it to prefer
        #  data, because we believe that's what we want to prefer,
        #  but left this note here in case things go haywire.
        # data = self.raw_data if self.raw_data is not None else self.data
        data = self.data if self.data is not None else self.raw_data
        width = data.shape[1]
        height = data.shape[0]
        xs = [ 0., width-1., 0., width-1. ]
        ys = [ 0., height-1., height-1., 0. ]
        scs = wcs.pixel_to_world( xs, ys )
        if isinstance( scs[0].ra, astropy.coordinates.Longitude ):
            ras = [ i.ra.to_value() for i in scs ]
            decs = [ i.dec.to_value() for i in scs ]
        else:
            ras = [ i.ra.value_in(u.deg).value for i in scs ]
            decs = [ i.dec.value_in(u.deg).value for i in scs ]
        ras, decs = FourCorners.sort_radec( ras, decs )
        self.ra_corner_00 = ras[0]
        self.ra_corner_01 = ras[1]
        self.ra_corner_10 = ras[2]
        self.ra_corner_11 = ras[3]
        self.minra = min( ras )
        self.maxra = max( ras )
        self.dec_corner_00 = decs[0]
        self.dec_corner_01 = decs[1]
        self.dec_corner_10 = decs[2]
        self.dec_corner_11 = decs[3]
        self.mindec = min( decs )
        self.maxdec = max( decs )

        if setradec:
            sc = wcs.pixel_to_world( data.shape[1] / 2., data.shape[0] / 2. )
            self.ra = sc.ra.to(u.deg).value
            self.dec = sc.dec.to(u.deg).value
            self.gallat = sc.galactic.b.deg
            self.gallon = sc.galactic.l.deg
            self.ecllat = sc.barycentrictrueecliptic.lat.deg
            self.ecllon = sc.barycentrictrueecliptic.lon.deg

    @classmethod
    def from_exposure(cls, exposure, section_id):
        """
        Copy the raw pixel values and relevant metadata from a section of an Exposure.

        Parameters
        ----------
        exposure: Exposure
            The exposure object to copy data from.
        section_id: int or str
            The part of the sensor to use when making this image.
            For single section instruments this is usually set to 0.

        Returns
        -------
        image: Image
            The new Image object. It would not have any data variables
            except for raw_data (and all the metadata values).
            To fill out data, flags, weight, etc., the application must
            apply the proper preprocessing tools (e.g., bias, flat, etc).

        """
        if not isinstance(exposure, Exposure):
            raise ValueError(f"The exposure must be an Exposure object. Got {type(exposure)} instead.")

        if exposure.id is None:
            raise RuntimeError( "Exposure id can't be none to use Image.from_exposure" )

        new = cls()

        new.exposure_id = exposure.id

        same_columns = [
            'type',
            'mjd',
            'end_mjd',
            'exp_time',
            'airmass',
            'instrument',
            'telescope',
            'filter',
            'project',
            'target',
            'format',
        ]

        # copy all the columns that are the same
        for column in same_columns:
            setattr(new, column, getattr(exposure, column))

        if exposure.filter_array is not None:
            idx = exposure.instrument_object.get_section_filter_array_index(section_id)
            new.filter = exposure.filter_array[idx]

        exposure.instrument_object.check_section_id(section_id)
        new.section_id = section_id
        new.raw_data = exposure.data[section_id]
        new.instrument_object = exposure.instrument_object

        # read the header from the exposure file's individual section data
        new._header = exposure.section_headers[section_id]

        # Because we will later be writing out float data (BITPIX=-32)
        # -- or whatever the type of raw_data is -- we have to make sure
        # there aren't any vestigal BSCALE and BZERO keywords in the
        # header.
        for delkw in [ 'BSCALE', 'BZERO' ]:
            if delkw in new.header:
                del new.header[delkw]

        # numpy array axis ordering is backwards from FITS ordering
        width = new.raw_data.shape[1]
        height = new.raw_data.shape[0]

        names = ['ra', 'dec'] + new.instrument_object.get_auxiliary_exposure_header_keys()
        header_info = new.instrument_object.extract_header_info(new._header, names)
        # TODO: get the important keywords translated into the searchable header column

        # figure out the RA/Dec of each image

        # first see if this instrument has a special method of figuring out the RA/Dec
        new.ra, new.dec = new.instrument_object.get_ra_dec_for_section(exposure, section_id)

        # Assume that if there's a WCS in the header, it's more reliable than the ra/dec keywords,
        #  (and more reliable than the function call above), so try that:
        try:
            new.set_corners_from_header_wcs( setradec=True )
        except:
            # If the WCS didn't work, and there was no special instrument method, then try other things

            # try to get the RA/Dec from the section header
            if new.ra is None or new.dec is None:
                new.ra = header_info.pop('ra', None)
                new.dec = header_info.pop('dec', None)

            # if we still have nothing, just use the RA/Dec of the global exposure
            # (Ideally, new.instrument_object.get_ra_dec_for_section will
            #  have used known chip offsets, so it will never come to this.)
            if new.ra is None or new.dec is None:
                new.ra = exposure.ra
                new.dec = exposure.dec

            new.calculate_coordinates()  # galactic and ecliptic coordinates

            # Set the corners
            halfwid = new.instrument_object.pixel_scale * width / 2. / np.cos( new.dec * np.pi / 180. ) / 3600.
            halfhei = new.instrument_object.pixel_scale * height / 2. / 3600.
            ra0 = new.ra - halfwid
            ra1 = new.ra + halfwid
            dec0 = new.dec - halfhei
            dec1 = new.dec + halfhei
            new.ra_corner_00 = ra0
            new.ra_corner_01 = ra0
            new.ra_corner_10 = ra1
            new.ra_corner_11 = ra1
            new.minra = ra0
            new.maxra = ra1
            new.dec_corner_00 = dec0
            new.dec_corner_01 = dec1
            new.dec_corner_10 = dec0
            new.dec_corner_11 = dec1
            new.mindec = dec0
            new.maxdec = dec1

        new.info = header_info  # save any additional header keys into a JSONB column


        return new

    @classmethod
    def copy_image(cls, image):
        """Make a new Image object with the same data as an existing Image object.

        This new object does not have a provenance or any relationships to other objects.
        It should be used only as a working copy, not to be saved back into the database.
        The filepath is set to None and should be manually set to a new (unique)
        value so as not to overwrite the original.
        """
        copy_attributes = cls.saved_extensions + [
            'header',
            'info',
        ]
        simple_attributes = [
            'ra',
            'dec',
            'gallon',
            'gallat',
            'ecllon',
            'ecllat',
            'mjd',
            'end_mjd',
            'exp_time',
            'instrument',
            'telescope',
            'filter',
            'section_id',
            'project',
            'target',
            'preproc_bitflag',
            'astro_cal_done',
            'sky_sub_done',
            'airmass',
            'fwhm_estimate',
            'zero_point_estimate',
            'lim_mag_estimate',
            'bkg_mean_estimate',
            'bkg_rms_estimate',
            'ref_image_id',
            'is_coadd',
            'is_sub',
            '_bitflag',
            '_upstream_bitflag',
            '_format',
            '_type',
        ]
        new = cls()
        for att in copy_attributes:
            value = getattr(image, att)
            if value is not None:
                setattr(new, att, value.copy())

        for att in simple_attributes:
            setattr(new, att, getattr(image, att))

        for axis in ['ra', 'dec']:
            for corner in ['00', '01', '10', '11']:
                setattr(new, f'{axis}_corner_{corner}', getattr(image, f'{axis}_corner_{corner}'))
            setattr( new, f'min{axis}', getattr( image, f'min{axis}' ) )
            setattr( new, f'max{axis}', getattr( image, f'max{axis}' ) )

        return new

    @classmethod
    def from_images(cls, images, index=0, set_is_coadd=True):
        """Create a new Image object from a list of other Image objects.

        This is the first step in making a multi-image (usually a
        coadd).  Do not use this to make subtractions!  Use
        from_ref_and_new instead.  Make sure to set the is_coadd flag of
        the returned image, as it's not set here (just in case there's
        some eventual usage other than making coadds).

        The output image doesn't have any data, and is created with
        nofile=True.  It is up to the calling application to fill in the data,
        flags, weight, etc. using the appropriate preprocessing tools.  It is
        also up to the calling application to fill in the image's provenance
        (which must include the provenances of the images that went into the
        combination as upstreams!).

        Parameters
        ----------
        images: list of Image objects
            The images to combine into a new Image object.

        index: int
            The image index in the (mjd sorted) list of upstream images
            that is used to set several attributes of the output image.
            Notably this includes the RA/Dec (and corners) of the output image,
            which implies that the indexed source image should be the one that
            all other images are aligned to (when running alignment).

        set_is_coadd: bool, default True
            Set the is_coadd field of the new image.  This is usually
            what you want, so that's the default.  Make this parameter
            False if for some reason you don't want the created image to
            flagged as a coadd.

        Returns
        -------
        output: Image
            The new Image object. It would not have any data variables or filepath.

        """
        if len(images) < 1:
            raise ValueError("Must provide at least one image to combine.")

        # sort images by mjd:
        images = sorted(images, key=lambda x: x.mjd)

        # Make sure input images all have ids set.  If these images were
        #   loaded from the database, then the ids will be set.  If they
        #   were created fresh, then they don't have ids yet, but
        #   accessing the id property will set them.  This does mean
        #   that the exact image objects passed need to be saved to the
        #   database when the coadded image is saved, so the ids track
        #   properly; otherwise, we'll end up with database integrity
        #   errors.  This is probably not an issue; in practical usage,
        #   most of the time we'll be coadding images from the database.
        #   When we won't is mostly going to be in tests where we don't
        #   want to save, or where we can control this.
        upstream_ids = [ i.id for i in images ]

        output = Image( nofile=True, is_coadd=set_is_coadd )

        fail_if_not_consistent_attributes = ['filter']
        copy_if_consistent_attributes = ['section_id', 'instrument', 'telescope', 'project', 'target', 'filter']
        copy_by_index_attributes = []
        for att in ['ra', 'dec']:
            copy_by_index_attributes.append(att)
            for corner in ['00', '01', '10', '11']:
                copy_by_index_attributes.append(f'{att}_corner_{corner}')
            copy_by_index_attributes.append( f'min{att}' )
            copy_by_index_attributes.append( f'max{att}' )

        copy_by_index_attributes += ['gallon', 'gallat', 'ecllon', 'ecllat']

        for att in fail_if_not_consistent_attributes:
            if len(set([getattr(image, att) for image in images])) > 1:
                raise ValueError(f"Cannot combine images with different {att} values: "
                                 f"{[getattr(image, att) for image in images]}")

        # only copy if attribute is consistent across upstreams, otherwise leave as None
        for att in copy_if_consistent_attributes:
            if len(set([getattr(image, att) for image in images])) == 1:
                setattr(output, att, getattr(images[0], att))

        # use the "index" to copy the attributes of that image to the output image
        for att in copy_by_index_attributes:
            setattr(output, att, getattr(images[index], att))

        # exposure time is usually added together
        output.exp_time = sum([image.exp_time for image in images])

        # start MJD and end MJD
        output.mjd = images[0].mjd  # assume sorted by start of exposures
        output.end_mjd = max([image.end_mjd for image in images])  # exposure ends are not necessarily sorted

        # TODO: what about the header? should we combine them somehow?
        output.info = images[index].info
        output.header = images[index].header

        base_type = images[index].type
        if not base_type.startswith('Com'):
            output.type = 'Com' + base_type

        output._upstream_ids = upstream_ids

        # mark as the reference the image used for alignment
        output.ref_image_id = images[index].id

        output._upstream_bitflag = 0
        for im in images:
            output._upstream_bitflag |= im.bitflag

        # Note that "data" is not filled by this method, also the provenance is empty!
        return output

    @classmethod
    def from_ref_and_new(cls, ref_image, new_image):
        return cls.from_new_and_ref(new_image, ref_image)

    @classmethod
    def from_new_and_ref(cls, new_image, ref_image):
        """Create a new Image object from a reference Image object and a new Image object.
        This is the first step in making a difference image.

        The output image doesn't have any data, and is created with
        nofile=True.  It is up to the calling application to fill in the
        data, flags, weight, etc. using the appropriate preprocessing tools.

        The Image objects used as inputs must have their own data products
        loaded before calling this method, so their provenances will be recorded.
        The provenance of the output object should be generated, then a call to
        output.provenance.upstreams = output.get_upstream_provenances()
        will make sure the provenance has the correct upstreams.

        After that, the data needs to be saved to file, and only then
        can the new Image be added to the database.

        Parameters
        ----------
        new_image: Image object
            The new image to use.
        ref_image: Image object
            The reference image to use.

        Returns
        -------
        output: Image
            The new Image object. It would not have any data variables or filepath.
        """


        if ref_image is None:
            raise ValueError("Must provide a reference image.")
        if new_image is None:
            raise ValueError("Must provide a new image.")

        ref_image_id = ref_image.id
        new_image_id = new_image.id

        output = Image(nofile=True)

        # for each attribute, check the two images have the same value
        for att in ['instrument', 'telescope', 'project', 'section_id', 'filter', 'target']:
            ref_value = getattr(ref_image, att)
            new_value = getattr(new_image, att)

            if att == 'section_id':
                ref_value = str(ref_value)
                new_value = str(new_value)

            # TODO: should replace this condition with a check that RA and Dec are overlapping?
            #  in that case: what do we consider close enough? how much overlap is reasonable?
            #  another issue: what happens if the section_id is different, what would be the
            #  value for the subtracted image? can it live without a value entirely?
            #  the same goes for target. what about coadded images? can they have no section_id??
            if att in ['section_id', 'filter', 'target'] and ref_value != new_value:
                raise ValueError(
                    f"Cannot combine images with different {att} values: "
                    f"{ref_value} and {new_value}. "
                )

            # assign the values from the new image
            setattr(output, att, new_value)

        fail_if_not_consistent_attributes = ['filter']

        for att in fail_if_not_consistent_attributes:
            if getattr(ref_image, att) != getattr(new_image, att):
                raise ValueError(f"Cannot combine images with different {att} values: "
                                 f"{getattr(ref_image, att)} and {getattr(new_image, att)}")

        if ref_image.mjd < new_image.mjd:
            output._upstream_ids = [ ref_image_id, new_image_id ]
        else:
            output._upstream_ids = [ new_image_id, ref_image_id ]

        output.ref_image_id = ref_image.id

        output._upstream_bitflag = 0
        output._upstream_bitflag |= ref_image.bitflag
        output._upstream_bitflag |= new_image.bitflag

        # get some more attributes from the new image
        for att in ['section_id', 'instrument', 'telescope', 'project', 'target',
                    'exp_time', 'airmass', 'mjd', 'end_mjd', 'info', 'header',
                    'gallon', 'gallat', 'ecllon', 'ecllat', 'ra', 'dec',
                    'ra_corner_00', 'ra_corner_01', 'ra_corner_10', 'ra_corner_11',
                    'dec_corner_00', 'dec_corner_01', 'dec_corner_10', 'dec_corner_11',
                    'minra', 'maxra', 'mindec', 'maxdec' ]:
            output.__setattr__(att, getattr(new_image, att))

        output.type = 'Diff'
        if new_image.type.startswith('Com'):
            output.type = 'ComDiff'

        output.is_sub = True

        # Note that "data" is not filled by this method, also the provenance is empty!
        return output

    def _make_aligned_images(self):
        """Align the upstream_images to one of the images pointed to by image_index.

        The parameters of the alignment must be given in the parameters attribute
        of this Image's Provenance.

        The index to which the images are aligned is given by the "to_index" key in the
        "alignment" dictionary in the parameters of the image provenance; the value can
        be "first" or "last".

        The resulting images are saved in _aligned_images, which are not saved
        to the database. Note that each aligned image is also referred to by
        a global variable under the ImageAligner.temp_images list.
        """
        raise RunTimeError( "Deprecated" )

        from improc.alignment import ImageAligner  # avoid circular import
        if self.provenance is None or self.provenance.parameters is None:
            raise RuntimeError('Cannot align images without a Provenance with legal parameters!')
        if 'alignment' not in self.provenance.parameters:
            raise RuntimeError('Cannot align images without an "alignment" dictionary in the Provenance parameters!')

        to_index = self.provenance.parameters['alignment'].get('to_index')
        if to_index == 'first':
            alignment_target = self.upstream_images[0]
        elif to_index == 'last':
            alignment_target = self.upstream_images[-1]
        elif to_index == 'new':
            alignment_target = self.new_image  # only works for a subtraction (or a coadd with exactly 2 upstreams)
        elif to_index == 'ref':
            alignment_target = self.ref_image  # this is not recommended!
        else:
            raise RuntimeError(
                f'Got illegal value for "to_index" ({to_index}) in the Provenance parameters!'
            )

        if self._aligner is None:
            self._aligner = ImageAligner(**self.provenance.parameters['alignment'])
        else:
            self._aligner.pars.override(self.provenance.parameters['alignment'])

        # verify all products are loaded
        for im in self.upstream_images:
            if im.sources is None or im.bg is None or im.wcs is None or im.zp is None:
                raise RuntimeError('Some images are missing data products. Try running load_upstream_products().')

        aligned = []
        for i, image in enumerate(self.upstream_images):
            SCLogger.debug( f"Aligning {image.id} ({image.filepath})" )
            new_image = self._aligner.run(image, alignment_target)
            aligned.append(new_image)
            # ImageAligner.temp_images.append(new_image)  # keep track of all these images for cleanup purposes

        self._aligned_images = aligned

    def _check_aligned_images(self):
        """Check that the aligned_images loaded in this Image are consistent.

        The aligned_images must have the same provenance parameters as the Image,
        and their "original_image_id" must point to the IDs of the upstream_images.

        If they are inconsistent, they will be removed and the _aligned_images
        attribute will be set to None to be lazy filled by _make_aligned_images().
        """
        raise RuntimeError( "Deprecated" )
        # if self._aligned_images is None:
        #     return

        # if self.provenance is None or self.provenance.parameters is None:
        #     raise RuntimeError('Cannot check aligned images without a Provenance with legal parameters!')
        # if 'alignment' not in self.provenance.parameters:
        #     raise RuntimeError(
        #         'Cannot check aligned images without an "alignment" dictionary in the Provenance parameters!'
        #     )

        # upstream_images_filepaths = [image.filepath for image in self.upstream_images]

        # for image in self._aligned_images:
        #     # im_pars will contain all the default keys and any overrides from self.provenance
        #     im_pars = image.info.get('alignment_parameters', {})

        #     # if self.provenance has non-default values, or if im_pars are missing any keys, remake all of them
        #     for key, value in self.provenance.parameters['alignment'].items():
        #         if key not in im_pars or im_pars[key] != value:
        #             self._aligned_images = None
        #             return

        #     if image.info['original_image_filepath'] not in upstream_images_filepaths:
        #         self._aligned_images = None
        #         return

    def _get_alignment_target_image(self):
        """Get the image in upstream_images that is the target to which we align all other images. """
        raise RuntimeError( "Deprecated" )

        # if self.provenance is None or self.provenance.parameters is None:
        #     raise RuntimeError('Cannot get alignment target without a Provenance with legal parameters!')
        # if 'alignment' not in self.provenance.parameters:
        #     raise RuntimeError(
        #         'Cannot get alignment target without an "alignment" dictionary in the Provenance parameters!'
        #     )

        # to_index = self.provenance.parameters['alignment'].get('to_index')
        # if to_index == 'first':
        #     alignment_target = self.upstream_images[0]
        # elif to_index == 'last':
        #     alignment_target = self.upstream_images[-1]
        # elif to_index == 'new':
        #     alignment_target = self.new_image
        # elif to_index == 'ref':
        #     alignment_target = self.ref_image
        # else:
        #     raise RuntimeError(
        #         f'Got illegal value for "to_index" ({to_index}) in the Provenance parameters!'
        #     )

        # return alignment_target

    def set_coordinates_to_match_target( self, target ):
        """Make sure the coordinates (RA,dec, corners and WCS) all match the alignment target image. """

        for att in ['ra', 'dec',
                    'ra_corner_00', 'ra_corner_01', 'ra_corner_10', 'ra_corner_11',
                    'dec_corner_00', 'dec_corner_01', 'dec_corner_10', 'dec_corner_11',
                    'minra', 'maxra', 'mindec', 'maxdec' ]:
            self.__setattr__(att, getattr(target, att))

    # @property
    # def aligned_images(self):
    #     """A set of images matching the upstream_images, only aligned (warped) to one of the image. """
    #     self._check_aligned_images()  # possibly destroy the old aligned images

    #     if self._aligned_images is None:
    #         self._make_aligned_images()

    #     return self._aligned_images

    # @aligned_images.setter
    # def aligned_images(self, value):
    #     self._aligned_images = value

    @property
    def instrument_object(self):
        if self.instrument is not None:
            if self._instrument_object is None or self._instrument_object.name != self.instrument:
                self._instrument_object = get_instrument_instance(self.instrument)

        return self._instrument_object

    @instrument_object.setter
    def instrument_object(self, value):
        self._instrument_object = value

    @property
    def filter_short(self):
        """The name of the filtered, shortened for display and for filenames. """
        if self.filter is None:
            return None
        return self.instrument_object.get_short_filter_name(self.filter)

    def __repr__(self):

        output = (
            f"Image(id: {self.id}, "
            f"type: {self.type}, "
            f"exp: {self.exp_time}s, "
            f"filt: {self.filter_short}, "
            f"from: {self.instrument}/{self.telescope} {self.section_id}, "
            f"filepath: {self.filepath}"
        )

        output += ")"

        return output

    def __str__(self):
        return self.__repr__()

    def invent_filepath(self):
        """Create a relative file path for the object.

        Create a file path relative to data root for the object based on its
        metadata.  This is used when saving the image to disk.  Data
        products that depend on an image and are also saved to disk
        (e.g., SourceList) will just append another string to the Image
        filename.

        Coadded or difference images (that have a list of upstream_images)
        will also be appended a "u-tag" which is just the letter u
        (for "upstreams") follwed by the first 6 characters of the
        SHA256 hash of the upstream image filepaths.  This is to make
        sure that the filepath is unique for each combination of
        upstream images.

        """
        prov_hash = inst_name = im_type = date = time = filter = ra = dec = dec_int_pm = ''
        section_id = section_id_int = ra_int = ra_int_h = ra_frac = dec_int = dec_frac = 0

        if self.provenance_id is not None:
            prov_hash = self.provenance_id
        if self.instrument_object is not None:
            inst_name = self.instrument_object.get_short_instrument_name()
        if self.type is not None:
            im_type = self.type

        if self.mjd is not None:
            t = Time(self.mjd, format='mjd', scale='utc').datetime
            date = t.strftime('%Y%m%d')
            time = t.strftime('%H%M%S')

        if self.filter_short is not None:
            filter = self.filter_short

        if self.section_id is not None:
            section_id = str(self.section_id)
            try:
                section_id_int = int(self.section_id)
            except ValueError:
                section_id_int = 0  # TODO: maybe replace with a placeholder like 99?

        if self.ra is not None:
            ra = self.ra
            ra_int, ra_frac = str(float(ra)).split('.')
            ra_int = int(ra_int)
            ra_int_h = ra_int // 15
            ra_frac = int(ra_frac)

        if self.dec is not None:
            dec = self.dec
            dec_int, dec_frac = str(float(dec)).split('.')
            dec_int = int(dec_int)
            dec_int_pm = f'p{dec_int:02d}' if dec_int >= 0 else f'm{-dec_int:02d}'
            dec_frac = int(dec_frac)

        cfg = config.Config.get()
        default_convention = "{inst_name}_{date}_{time}_{section_id}_{filter}_{im_type}_{prov_hash:.6s}"
        name_convention = cfg.value('storage.images.name_convention', default=None)
        if name_convention is None:
            name_convention = default_convention

        filepath = name_convention.format(
            inst_name=inst_name,
            im_type=im_type,
            date=date,
            time=time,
            filter=filter,
            ra=ra,
            ra_int=ra_int,
            ra_int_h=ra_int_h,
            ra_frac=ra_frac,
            dec=dec,
            dec_int=dec_int,
            dec_int_pm=dec_int_pm,
            dec_frac=dec_frac,
            section_id=section_id,
            section_id_int=section_id_int,
            prov_hash=prov_hash,
        )

        # TODO: which elements of the naming convention are really necessary?
        #  and what is a good way to make sure the filename actually depends on them?

        if self._upstream_ids is not None and len(self._upstream_ids) > 0:
            utag = hashlib.sha256()
            for id in self._upstream_ids:
                utag.update( str(id).encode('utf-8') )
            utag = base64.b32encode(utag.digest()).decode().lower()
            utag = '_u-' + utag[:6]
            filepath += utag
            # ignore situations where upstream_images is not loaded, it should not happen for a combined image

        return filepath

    def save(self, filename=None, only_image=False, just_update_header=True, **kwargs ):
        """Save the data (along with flags, weights, etc.) to disk.
        The format to save is determined by the config file.
        Use the filename to override the default naming convention.

        Will save the standard image extensions : image, weight, mask.
        Does not save the source list or psf or other things that have
        their own objects; those need to be saved separately.  (Also see
        pipeline.datastore.)

        Parameters
        ----------
        filename: str (optional)
            The filename to use to save the data.  If not provided, will
            use what is in self.filepath; if that is None, then the
            default naming convention willbe used.  self.filepath will
            be updated to this name

        only_image: bool, default False
            If the image is stored as multiple files (i.e. image,
            weight, and flags extensions are all stored as seperate
            files, rather than as HDUs within one file), then _only_
            write the image out.  The use case for this is for
            "first-look" headers with astrometric and photometric
            solutions; the image header gets updated in that case, but
            the weight and flags files stay the same, so they do not
            need to be updated.  You will usually want just_update_header
            to be True when only_image is True.

        just_update_header: bool, default True
            Ignored unless only_image is True and the image is stored as
            multiple files rather than as FITS extensions.  In this
            case, if just_udpate_header is True and the file already
            exists, don't write the data, just update the header.

        **kwargs: passed on to FileOnDiskMixin.save(), include:
            overwrite - bool, set to True if it's OK to overwrite exsiting files
            no_archive - bool, set to True to save only to local disk, otherwise also saves to the archive
            exists_ok, verify_md5 - complicated, see documentation on FileOnDiskMixin

        For images being saved to the database, you probably want to use
        overwrite=True, verify_md5=True, or perhaps overwrite=False,
        exists_ok=True, verify_md5=True.  For temporary images being
        saved as part of local processing, you probably want to use
        verify_md5=False and either overwrite=True (if you're modifying
        and writing the file multiple times), or overwrite=False,
        exists_ok=True (if you might call the save() method more than
        once on the same image, and you want to trust the filesystem to
        have saved it right).

        """
        if self.data is None:
            raise RuntimeError("The image data is not loaded. Cannot save.")

        if self.provenance_id is None:
            raise RuntimeError("The image provenance is not set. Cannot save.")

        if filename is not None:
            self.filepath = filename
        if self.filepath is None:
            self.filepath = self.invent_filepath()

        cfg = config.Config.get()
        single_file = cfg.value('storage.images.single_file', default=False)
        format = cfg.value('storage.images.format', default='fits')
        extensions = []
        files_written = {}

        if not only_image:
            # In order to ignore just_update_header if only_image is false,
            # we need to pass it as False on to save_fits_image_file
            just_update_header = False

        full_path = os.path.join(self.local_path, self.filepath)

        if format == 'fits':
            # save the imaging data
            extensions.append('.image.fits')
            imgpath = save_fits_image_file(full_path, self.data, self.header,
                                           extname='image', single_file=single_file,
                                           just_update_header=just_update_header)
            files_written['.image.fits'] = imgpath
            # TODO: we can have extensions at the end of the self.filepath (e.g., foo.fits.flags)
            #  or we can have the extension name carry the file extension (e.g., foo.flags.fits)
            #  this should be configurable and will affect how we make the self.filepath and extensions.

            # save the other extensions
            if single_file or ( not only_image ):
                for array_name in self.saved_extensions:
                    if array_name == 'data':
                        continue
                    array = getattr(self, array_name)
                    if array is not None:
                        extpath = save_fits_image_file(
                            full_path,
                            array,
                            self.header,
                            extname=array_name,
                            single_file=single_file
                        )
                        array_name = '.' + array_name
                        if not array_name.endswith('.fits'):
                            array_name += '.fits'
                        extensions.append(array_name)
                        if not single_file:
                            files_written[array_name] = extpath

            if single_file:
                files_written = files_written['.image.fits']
                if not self.filepath.endswith('.fits'):
                    self.filepath += '.fits'

        elif format == 'hdf5':
            # TODO: consider writing a more generic utility to save_image_file that handles either fits or hdf5, etc.
            raise NotImplementedError("HDF5 format is not yet supported.")
        else:
            raise ValueError(f"Unknown image format: {format}. Use 'fits' or 'hdf5'.")

        # Save the file to the archive and update the database record
        # (as well as self.filepath, self.filepath_extensions, self.md5sum, self.md5sum_extensions)
        # (From what we did above, it's already in the right place in the local filestore.)
        if single_file:
            FileOnDiskMixin.save( self, files_written, **kwargs )
        else:
            if just_update_header:
                FileOnDiskMixin.save( self, files_written['.image.fits'], '.image.fits', **kwargs )
            else:
                for ext in extensions:
                    FileOnDiskMixin.save( self, files_written[ext], ext, **kwargs )

    def load(self):
        """Load the image data from disk.
        This includes the _data property,
        but can also load the _flags, _weight,
        _background, _score, _psfflux and _psffluxerr properties.

        """

        if self.filepath is None:
            raise ValueError("The filepath is not set. Cannot load the image.")

        cfg = config.Config.get()
        single_file = cfg.value('storage.images.single_file')

        if single_file:
            filename = self.get_fullpath()
            if not os.path.isfile(filename):
                raise FileNotFoundError(f"Could not find the image file: {filename}")
            self._data, self._header = read_fits_image(filename, ext='image', output='both')
            for att in self.saved_extensions:
                if att == 'data':
                    continue
                array = read_fits_image(filename, ext=att)
                setattr(self, f'_{att}', array)

        else:  # load each data array from a separate file
            if self.filepath_extensions is None:
                self._data, self._header = read_fits_image( self.get_fullpath(), output='both' )
            else:
                gotim = False
                gotweight = False
                gotflags = False
                for extension, filename in zip( self.filepath_extensions, self.get_fullpath(as_list=True) ):
                    if not os.path.isfile(filename):
                        raise FileNotFoundError(f"Could not find the image extension file: {filename}")
                    if extension == '.image.fits':
                        self._data, self._header = read_fits_image(filename, output='both')
                        gotim = True
                    elif extension == '.weight.fits':
                        self._weight = read_fits_image(filename, output='data')
                        gotweight = True
                    elif extension == '.flags.fits':
                        self._flags = read_fits_image(filename, output='data')
                        gotflags = True
                    else:  # other extensions like score and psfflux
                        stripped_ext = extension
                        if stripped_ext.startswith('.'):
                            stripped_ext = stripped_ext[1:]
                        if stripped_ext.endswith('.fits'):
                            stripped_ext = stripped_ext[:-5]
                        if stripped_ext in self.saved_extensions:
                            # e.g., for extension .score.fits, self._score = read_fits_image(filename, output='data')
                            setattr(self, f'_{stripped_ext}', read_fits_image(filename, output='data'))
                        else:
                            raise ValueError( f'Unknown image extension {extension}' )

                if not ( gotim and gotweight and gotflags ):
                    raise FileNotFoundError( "Failed to load at least one of image, weight, flags" )

    def free( self, only_free=None ):
        """Free loaded image memory.  Does not delete anything from disk.

        Will wipe out any loaded image, weight, flags, background,
        score, psfflux, psffluxerr, nandata, and nanscore data, for
        purposes of saving memory.  Doesn't make sure anything is saved
        to disk, so only use this when you know you can use it.

        (This is accomplished by setting the parameters of self that
        store that data to None, and otherwise depends on the python
        garbage collector.  If there are other references to the data
        pointed to by those parameters, the memory of course won't
        actually be freed.)

        Parameters
        ----------
          only_free: set or list of strings
             If you pass this string, it will not free everything, but
             only the things you specify here.  Members of the string
             can include raw_data, data, weight, flags, background, score,
             psfflux, psffluxerr, nandata, and nanscore.

        """
        allfree = set( Image.saved_extensions )
        allfree.add( "raw_data" )
        tofree = set( only_free ) if only_free is not None else allfree

        for prop in tofree:
            if prop not in allfree:
                raise RuntimeError( f"Unknown image property to free: {prop}" )
            if prop == 'raw_data':
                self.raw_data = None
            else:
                setattr( self, f'_{prop}', None )


    def load_products(self, provenances, session=None, must_find_all=True):
        """Load the products associated with this image, using a list of provenances.

        Parameters
        ----------
        provenances: single Provenance or list of Provenance objects
            A list to go over, that can contain any number of Provenance objects.
            Will search the database for matching objects to each provenance in turn,
            and will assign them into "self" if found.
            Note that it will keep the first successfully loaded product on the provenance list.
            Will overwrite any existing products on the Image.
            Will ignore provenances that do not match any of the products
            (e.g., provenances for a different processing step).
        session: SQLAlchemy session, optional
            The session to use for the database queries.
            If not provided, will open a session internally
            and close it when the function exits.

        """
        raise RuntimeError( "Don't use this, data products are in DataStore" )


    def get_upstream_provenances(self):
        """Collect the provenances for all upstream objects.

        This does not recursively go back to the upstreams of the upstreams.
        It gets only the provenances of the immediate upstream objects.

        Provenances that are the same (have the same hash) are combined (returned only once).

        This is what would generally be put into a new provenance's upstreams list.

        Returns
        -------
        list of Provenance objects:
            A list of all the provenances for the upstream objects.
        """
        upstream_objs = self.get_upstreams()
        provids = [ i.provenance_id for i in upstream_objs ]
        provs = Provenance.get_batch( provids )
        return provs

    def get_upstream_products_for_alignment( self, myprov=None, session=None ):
        """Return everything needed from upstream images in order to perform alignment.

        Parameters
        ----------
          myprov: Provenance, or None
            Here so that you can work with a provenance not yet loaded
            into the database.  If this is None, will load the
            provenance defined by self.provenance_id.

        Returns
        -------
          images: list, sourceses: dict, bgs: dict, psfs: dict, wcses: dict, zps: dict
            image is a list of Image objects
            sourceses is a list of SourceImage objects, keyed by image_id
            bgs is a list of Background objects, keyed by sources_id
            psfs is a list of PSF objects, keyed by sources_id
            wcses is a list of WorldCorodinates objects, keyed by sources_id
            zps is a list of ZeroPoint objects, keyed by sources_id

        """
        raise RuntimeError( "I don't think this actually used." )
        images = []
        sourceses = []
        bgs = []
        psfs = []
        wcses = []
        zps = []

        if myprov is None:
            myprov = Provenance.get( self.provenance_id )
        upstrprov = myprov.get_upstreams()
        upstrprovids = [ i.id for i in upstrprov ]

        with SmartSession( session ) as session:
            images = session.query( Image ).filter( Image._id.in_( self.upstream_image_ids ) ).all()

            # Upstream images first
            upstrimages = session.query( Image ).filter( Image._id.in_( self.upstream_image_ids ) ).all()

            # Get all of the other falderal associated with those images
            upstrsources = ( session.query( SourceList )
                             .filter( SourceList.image_id ).in_( self.upstream_image_ids )
                             .filter( SourceList.provenance_id ).in_( upstrprovids )
                             .all() )
            upstrsrcids = [ s.id for s in upstrsources ]

            upstrbkgs = session.query( Background ).filter( Background.sources_id.in_( upstrsrcids ) ).all()
            upstrpsfs = session.query( PSF ).filter( PSF.sources_id.in_( upstsrsrcids ) ).all()
            upstrwcses = session.query( WorldCoordinate ).filter( WorldCoordinates.sources_id.in_( upstrsrcids ) ).all()
            upstrzps = session.query( ZeroPoint ).filter( ZeroPoint.sources_id.in_( upstrsrcids ) ).all()

        # Verify cleanliness
        upstrsrc_imageids = [ s.image_id for s in upstrsrcids ]
        if len( set( upstrsrc_imageids ) ) != len( upstrsrc_imageids ):
            raise RuntimeError( "Image.get_upstream_products_for_alignment found multiple matching "
                                "SourceLists for at least some upstream images." )
        if len( upstrsrc_imageids ) != len( upstrimages ):
            raise RuntimeError( "Image.get_upstream_products_for_alignment didn't find a SourceList "
                                "for every upstream image." )

        for other in [ upstrbkgs, upstrpsfs, upstrwcses, upstrzps ]:
            other_srcids = [ o.sources_id for o in other ]
            if len( set( other_srcids ) ) != len( other_srcids ):
                raise RuntimeError( "Image.get_upstream_products_for_alignment found multiple matching "
                                    "data products." )
            if len( other_srcids ) != len( upstrimages ):
                raise RuntimeError( "Image.get_upstream_products_for_alignment didn't find all "
                                    "data products for all images." )

        sourceses = { u.image_id: u for u in upstrsources }
        bgs = { u.sources_id: u for u in upstrbkgs }
        psfs = { u.sources_id: u for u in upstrpsfs }
        wcses = { u.sources_id: u for u in upstrscses }
        zps = { u.sourcers_id: u for u in upstrzps }

        return images, sourceses, bgs, psfs, wcses, zps


    def get_upstreams(self, only_images=False, session=None):
        """Get the upstream images and associated products that were used to make this image.

        This includes the reference/new image (for subtractions) or the set of
        images used to build a coadd.  Each image will have some products that
        were generated from it (source lists, PSFs, etc.) that also count as
        upstreams to this image.

        Not recursive.  (So, won't get the Exposure upstreams of the images
        that went into a coadd, for instance, and if by some chance you have a
        coadd of coadds (don't do that!), the images that went into the coadd
        that was coadded to produce this coadd won't be loaded.  (Got that?))

        Parameters
        ----------
        only_images: bool, default False
             If True, only get upstream images, not the other assorted data products.

        session: SQLAlchemy session (optional)
            The session to use to query the database.  If not provided,
            will open a new session that automatically closes at
            the end of the function.

        Returns
        -------
        upstreams: list of objects
            The upstream Exposure, Image, SourceList, Background, WCS,
            ZeroPoint, PSF objects that were used to create this image.  For most
            images, it will be (at most) a single Exposure.  For subtraction and
            coadd images, there could be all those other things.

        """

        # Avoid circular imports
        from models.source_list import SourceList
        from models.background import Background
        from models.psf import PSF
        from models.world_coordinates import WorldCoordinates
        from models.zero_point import ZeroPoint

        upstreams = []
        with SmartSession(session) as session:
            # Load the exposure if there is one
            if self.exposure_id is not None:
                upstreams.append( session.query( Exposure ).filter( self.exposure_id == Exposure._id ).first() )

            if ( not self.is_coadd ) and ( not self.is_sub ):
                # We're done!  That wasn't so bad.
                return upstreams

            # This *is* so bad....
            myprov = session.query( Provenance ).filter( self.provenance_id == Provenance._id ).first()
            upstrprov = myprov.get_upstreams()
            upstrprovids = [ i.id for i in upstrprov ]

            # Upstream images first
            upstrimages = session.query( Image ).filter( Image._id.in_( self.upstream_image_ids ) ).all()
            # Sort by mjd
            upstrimages.sort( key=lambda i: i.mjd )
            upstreams.extend( upstrimages )

            if not only_images:
                # Get all of the other falderal associated with those images
                upstrsources = ( session.query( SourceList )
                                 .filter( SourceList.image_id.in_( self.upstream_image_ids ) )
                                 .filter( SourceList.provenance_id.in_( upstrprovids ) )
                                 .all() )
                upstrsrcids = [ s.id for s in upstrsources ]

                upstrbkgs = session.query( Background ).filter( Background.sources_id.in_( upstrsrcids ) ).all()
                upstrpsfs = session.query( PSF ).filter( PSF.sources_id.in_( upstrsrcids ) ).all()
                upstrwcses = ( session.query( WorldCoordinates )
                               .filter( WorldCoordinates.sources_id.in_( upstrsrcids ) ) ).all()
                upstrzps = session.query( ZeroPoint ).filter( ZeroPoint.sources_id.in_( upstrsrcids ) ).all()

                upstreams.extend( list(upstrsources) )
                upstreams.extend( list(upstrbkgs) )
                upstreams.extend( list(upstrpsfs) )
                upstreams.extend( list(upstrwcses) )
                upstreams.extend( list(upstrzps) )

        return upstreams

    def get_downstreams(self, session=None, only_images=False, siblings=False):
        """Get all the objects that were created based on this image. """

        # avoids circular import
        from models.source_list import SourceList
        from models.psf import PSF
        from models.background import Background
        from models.world_coordinates import WorldCoordinates
        from models.zero_point import ZeroPoint

        downstreams = []
        with SmartSession(session) as session:
            if not only_images:
                # get all source lists that are related to this image (regardless of provenance)
                sources = session.scalars( sa.select(SourceList).where(SourceList.image_id == self.id) ).all()
                downstreams.extend( list(sources) )
                srcids = [ s.id for s in sources ]

                # Get the bkgs, psfs, wcses, and zps assocated with all of those sources
                bkgs = session.query( Background ).filter( Background.sources_id.in_( srcids ) ).all()
                psfs = session.query( PSF ).filter( PSF.sources_id.in_( srcids ) ).all()
                wcses = session.query( WorldCoordinates ).filter( WorldCoordinates.sources_id.in_( srcids ) ).all()
                zps = session.query( ZeroPoint ).filter( ZeroPoint.sources_id.in_( srcids ) ).all()

                downstreams.extend( list(bkgs) )
                downstreams.extend( list(psfs) )
                downstreams.extend( list(wcses) )
                downstreams.extend( list(zps) )

            # Now get all images that are downstream of this image.

            dsimgs = ( session.query( Image )
                       .join( image_upstreams_association_table,
                              image_upstreams_association_table.c.downstream_id == Image._id )
                       .filter( image_upstreams_association_table.c.upstream_id == self.id )
                      ).all()
            downstreams.extend( list(dsimgs) )

        return downstreams

    @staticmethod
    def query_images(
            ra=None,
            dec=None,
            target=None,
            section_id=None,
            project=None,
            instrument=None,
            filter=None,
            min_mjd=None,
            max_mjd=None,
            min_dateobs=None,
            max_dateobs=None,
            min_exp_time=None,
            max_exp_time=None,
            min_seeing=None,
            max_seeing=None,
            min_lim_mag=None,
            max_lim_mag=None,
            min_airmass=None,
            max_airmass=None,
            min_background=None,
            max_background=None,
            min_zero_point=None,
            max_zero_point=None,
            order_by='latest',
            seeing_quality_factor=3.0,
            provenance_ids=None,
            type=[1, 2, 3, 4],  # TODO: is there a smarter way to only get science images?
    ):
        """Get a SQL alchemy statement object for Image objects, with some filters applied.

        This is a convenience method to get a statement object that can be further filtered.
        If no parameters are given, will happily return all images (be careful with this).
        It is highly recommended to supply ra/dec to find all images overlapping with that point.

        The images are sorted either by MJD or by image quality.
        Quality is defined as sum of the limiting magnitude and the seeing,
        multiplied by the negative "seeing_quality_factor" parameter:
          <quality> = <limiting_mag> - <seeing_quality_factor> * <seeing FWHM>
        This means that as the seeing FWHM is smaller, and the limiting magnitude
        is bigger (fainter) the quality is higher.
        Choose a higher seeing_quality_factor to give more weight to the seeing,
        and less weight to the limiting magnitude.

        Parameters
        ----------
        ra: float or str (optional)
            The right ascension of the target in degrees or in HMS format.
            Will find all images that contain this position.
            If given, must also give dec.
        dec: float or str (optional)
            The declination of the target in degrees or in DMS format.
            Will find all images that contain this position.
            If given, must also give ra.
        target: str or list of strings (optional)
            Find images that have this target name (e.g., field ID or Object name).
            If given as a list, will match all the target names in the list.
        section_id: int/str or list of ints/strings (optional)
            Find images with this section ID.
            If given as a list, will match all the section IDs in the list.
        project: str or list of strings (optional)
            Find images from this project.
            If given as a list, will match all the projects in the list.
        instrument: str or list of str (optional)
            Find images taken using this instrument.
            Provide a list to match multiple instruments.
        filter: str or list of str (optional)
            Find images taken using this filter.
            Provide a list to match multiple filters.
        min_mjd: float (optional)
            Find images taken after this MJD.
        max_mjd: float (optional)
            Find images taken before this MJD.
        min_dateobs: str (optional)
            Find images taken after this date (use ISOT format or a datetime object).
        max_dateobs: str (optional)
            Find images taken before this date (use ISOT format or a datetime object).
        min_exp_time: float (optional)
            Find images with exposure time longer than this (in seconds).
        max_exp_time: float (optional)
            Find images with exposure time shorter than this (in seconds).
        min_seeing: float (optional)
            Find images with seeing FWHM larger than this (in arcsec).
        max_seeing: float (optional)
            Find images with seeing FWHM smaller than this (in arcsec).
        min_lim_mag: float (optional)
            Find images with limiting magnitude larger (fainter) than this.
        max_lim_mag: float (optional)
            Find images with limiting magnitude smaller (brighter) than this.
        min_airmass: float (optional)
            Find images with airmass larger than this.
        max_airmass: float (optional)
            Find images with airmass smaller than this.
        min_background: float (optional)
            Find images with background rms higher than this.
        max_background: float (optional)
            Find images with background rms lower than this.
        min_zero_point: float (optional)
            Find images with zero point higher than this.
        max_zero_point: float (optional)
            Find images with zero point lower than this.
        order_by: str, default 'latest'
            Sort the images by 'earliest', 'latest' or 'quality'.
            The 'earliest' and 'latest' order by MJD, in ascending/descending order, respectively.
            The 'quality' option will try to order the images by quality, as defined above,
            with the highest quality images first.
        seeing_quality_factor: float, default 3.0
            The factor to multiply the seeing FWHM by in the quality calculation.
        provenance_ids: str or list of strings
            Find images with these provenance IDs.
        type: integer or string or list of integers or strings, default [1,2,3,4]
            List of integer converted types of images to search for.
            This defaults to [1,2,3,4] which corresponds to the
            science images, coadds and subtractions
            (see enums_and_bitflags.ImageTypeConverter for more details).
            Choose 1 to get only the regular (non-coadd, non-subtraction) images.

        Returns
        -------
        stmt: SQL alchemy select statement
            The statement to be executed to get the images.
            Do session.scalars(stmt).all() to get the images.
            Additional filtering can be done on the statement before executing it.
        """
        stmt = sa.select(Image)

        # filter by coordinates being contained in the image
        if ( ra is not None ) or ( dec is not None ):
            raise RuntimeError( "Image.query_images using ra and dec is not efficient.  Don't use it for now. "
                                "TODO: write Image.find_images that does the same thing, but just returns a "
                                "list of images rather than an SQLA query, and that can handle everything." )
        # if ra is not None and dec is not None:
        #     if isinstance(ra, str):
        #         ra = parse_ra_hms_to_deg(ra)
        #     if isinstance(dec, str):
        #         dec = parse_dec_dms_to_deg(dec)
        #     stmt = stmt.where(Image.containing(ra, dec))
        # elif ra is not None or dec is not None:
        #     raise ValueError("Both ra and dec must be provided to search by position.")

        # filter by target (e.g., field ID, object name) and possibly section ID and/or project
        targets = listify(target)
        if targets is not None:
            stmt = stmt.where(Image.target.in_(targets))
        section_ids = listify(section_id)
        if section_ids is not None:
            stmt = stmt.where(Image.section_id.in_(section_ids))
        projects = listify(project)
        if projects is not None:
            stmt = stmt.where(Image.project.in_(projects))

        # filter by filter and instrument
        filters = listify(filter)
        if filters is not None:
            stmt = stmt.where(Image.filter.in_(filters))
        instruments = listify(instrument)
        if instruments is not None:
            stmt = stmt.where(Image.instrument.in_(instruments))

        # filter by MJD or dateobs
        if min_mjd is not None:
            if min_dateobs is not None:
                raise ValueError("Cannot filter by both minimal MJD and dateobs.")
            stmt = stmt.where(Image.mjd >= min_mjd)
        if max_mjd is not None:
            if max_dateobs is not None:
                raise ValueError("Cannot filter by both maximal MJD and dateobs.")
            stmt = stmt.where(Image.mjd <= max_mjd)
        if min_dateobs is not None:
            min_dateobs = parse_dateobs(min_dateobs, output='mjd')
            stmt = stmt.where(Image.mjd >= min_dateobs)
        if max_dateobs is not None:
            max_dateobs = parse_dateobs(max_dateobs, output='mjd')
            stmt = stmt.where(Image.mjd <= max_dateobs)

        # filter by exposure time
        if min_exp_time is not None:
            stmt = stmt.where(Image.exp_time >= min_exp_time)
        if max_exp_time is not None:
            stmt = stmt.where(Image.exp_time <= max_exp_time)

        # filter by seeing FWHM
        if min_seeing is not None:
            stmt = stmt.where(Image.fwhm_estimate >= min_seeing)
        if max_seeing is not None:
            stmt = stmt.where(Image.fwhm_estimate <= max_seeing)

        # filter by limiting magnitude
        if max_lim_mag is not None:
            stmt = stmt.where(Image.lim_mag_estimate <= max_lim_mag)
        if min_lim_mag is not None:
            stmt = stmt.where(Image.lim_mag_estimate >= min_lim_mag)

        # filter by airmass
        if max_airmass is not None:
            stmt = stmt.where(Image.airmass <= max_airmass)
        if min_airmass is not None:
            stmt = stmt.where(Image.airmass >= min_airmass)

        # filter by background
        if max_background is not None:
            stmt = stmt.where(Image.bkg_rms_estimate <= max_background)
        if min_background is not None:
            stmt = stmt.where(Image.bkg_rms_estimate >= min_background)

        # filter by zero point
        if max_zero_point is not None:
            stmt = stmt.where(Image.zero_point_estimate <= max_zero_point)
        if min_zero_point is not None:
            stmt = stmt.where(Image.zero_point_estimate >= min_zero_point)

        # filter by provenances
        provenance_ids = listify(provenance_ids)
        if provenance_ids is not None:
            stmt = stmt.where(Image.provenance_id.in_(provenance_ids))

        # filter by image types
        types = listify(type)
        if types is not None:
            int_types = [ImageTypeConverter.to_int(t) for t in types]
            stmt = stmt.where(Image._type.in_(int_types))

        # sort the images
        if order_by == 'earliest':
            stmt = stmt.order_by(Image.mjd)
        elif order_by == 'latest':
            stmt = stmt.order_by(sa.desc(Image.mjd))
        elif order_by == 'quality':
            stmt = stmt.order_by(
                sa.desc(Image.lim_mag_estimate - abs(seeing_quality_factor) * Image.fwhm_estimate)
            )
        else:
            raise ValueError(f'Unknown order_by parameter: {order_by}. Use "earliest", "latest" or "quality".')

        return stmt

    @staticmethod
    def get_image_from_upstreams(images, prov_id=None, session=None):
        """Finds the combined image that was made from exactly the list of images (with a given provenance). """

        with SmartSession(session) as session:
            association = image_upstreams_association_table

            stmt = ( sa.select(Image)
                     .join( association, Image._id == association.c.downstream_id )
                     .where( association.c.upstream_id.in_( [ i.id for i in images ] ) )
                    ).group_by( Image._id )

            if prov_id is not None:  # pick only those with the right provenance id
                if isinstance(prov_id, Provenance):
                    prov_id = prov_id.id
                stmt = stmt.where(Image.provenance_id == prov_id)

            output = session.scalars(stmt).all()
            if len(output) > 1:
                raise ValueError(
                    f"More than one combined image found with provenance ID {prov_id} and upstreams {images}."
                )
            elif len(output) == 0:
                return None

            return output[0]  # should usually return one Image or None

    @property
    def data(self):
        """The underlying pixel data array (2D float array). """
        if self._data is None and self.filepath is not None:
            self.load()
        return self._data

    @data.setter
    def data(self, value):
        self._nandata = None
        self._data = value

    @property
    def header(self):
        if self._header is None and self.filepath is not None:
            self.load()
        if self._header is None:
            self._header = fits.Header()
        return self._header

    @header.setter
    def header(self, value):
        if not isinstance(value, fits.Header):
            raise ValueError(f"data must be a fits.Header object. Got {type(value)} instead. ")
        self._header = value

    @property
    def flags(self):
        """The bit-flag array (2D int array). """
        if self._data is None and self.filepath is not None:
            self.load()
        return self._flags

    @flags.setter
    def flags(self, value):
        self._nandata = None
        self._nanscore = None
        self._flags = value

    @property
    def weight(self):
        """The inverse-variance array (2D float array). """
        if self._data is None and self.filepath is not None:
            self.load()
        return self._weight

    @weight.setter
    def weight(self, value):
        self._weight = value

    @property
    def score(self):
        """The image after filtering with the PSF and normalizing to S/N units (2D float array). """
        if self._data is None and self.filepath is not None:
            self.load()
        return self._score

    @score.setter
    def score(self, value):
        self._nanscore = None
        self._score = value

    @property
    def psfflux(self):
        """An array containing an estimate for the (PSF-fitted) flux of each point in the image. """
        if self._data is None and self.filepath is not None:
            self.load()
        return self._psfflux

    @psfflux.setter
    def psfflux(self, value):
        self._psfflux = value

    @property
    def psffluxerr(self):
        """The error for each pixel of the PSF-fit flux array. """
        if self._data is None and self.filepath is not None:
            self.load()
        return self._psffluxerr

    @psffluxerr.setter
    def psffluxerr(self, value):
        self._psffluxerr = value

    @property
    def nandata(self):
        """The image data, only masked with NaNs wherever the flag is not zero. """
        if self._nandata is None:
            self._nandata = self.data.copy()
            if self.flags is not None:
                self._nandata[self.flags != 0] = np.nan
        return self._nandata

    @nandata.setter
    def nandata(self, value):
        self._nandata = value

    @property
    def nanscore(self):
        """The image data, only masked with NaNs wherever the flag is not zero. """
        if self._nanscore is None:
            self._nanscore = self.score.copy()
            if self.flags is not None:
                self._nanscore[self.flags != 0] = np.nan
        return self._nanscore

    @nanscore.setter
    def nanscore(self, value):
        self._nanscore = value

    @property
    def data_bgsub(self):
        """The image data, after subtracting the background. If no Background object is loaded, will raise. """
        raise RuntimeError( "Deprecated" )
        if self.bg is None:
            raise ValueError("No background is loaded for this image.")
        if self.bg.format == 'scalar':
            return self.data - self.bg.value
        else:
            return self.data - self.bg.counts

    @property
    def nandata_bgsub(self):
        """The image data, after subtracting the background and masking with NaNs wherever the flag is not zero. """
        raise RuntimeError( "Deprecated" )
        # if self.bg is None:
        #     raise ValueError("No background is loaded for this image.")
        # if self.bg.format == 'scalar':
        #     return self.nandata - self.bg.value
        # else:
        #     return self.nandata - self.bg.counts

    def show(self, **kwargs):
        """
        Display the image using the matplotlib imshow function.

        Parameters
        ----------
        **kwargs: passed on to matplotlib.pyplot.imshow()
            Additional keyword arguments to pass to imshow.
        """
        import matplotlib.pyplot as plt
        mu, sigma = sigma_clipping(self.data)
        defaults = {
            'cmap': 'gray',
            # 'origin': 'lower',
            'vmin': mu - 3 * sigma,
            'vmax': mu + 5 * sigma,
        }
        defaults.update(kwargs)
        plt.imshow(self.nandata, **defaults)

    # ======================================================================
    # The fields below are things that we've deprecated; these definitions
    #   are here to catch cases in the code where they're still used

    @property
    def provenance( self ):
        raise RuntimeError( "Don't use provenance, use provenance_id" )

    @provenance.setter
    def provenance( self, val ):
        raise RuntimeError( "Don't use provenance, use provenance_id" )

    @property
    def exposure( self ):
        raise RuntimeError( "Don't use exposure, use exposure_id" )

    @exposure.setter
    def exposure( self, val ):
        raise RuntimeError( "Don't use exposure, use exposure_id" )

    @property
    def upstream_images( self ):
        raise RuntimeError( "Don't use upstream_images, use get_upstreams" )

    @upstream_images.setter
    def upstream_images( self, val ):
        raise RuntimeError( "Don't use upstream_images, create image with from_images or from_ref_and_new" )

    @property
    def downstream_images( self ):
        raise RuntimeError( "Don't use downstream_images, use get_downstreams()" )

    @downstream_images.setter
    def downstream_images( self, val ):
        raise RuntimeError( "Can't set downstream images." )

    @property
    def ref_image( self ):
        raise RuntimeError( "Don't use ref_image, use ref_image_id" )

    @ref_image.setter
    def ref_image( self, val ):
        raise RuntimeError( "Don't use ref_image, use ref_image_id" )

    @property
    def new_image( self ):
        raise RuntimeError( "Don't use new_image, use new_image_id" )

    @new_image.setter
    def new_image( self, val ):
        raise RuntimeError( "Don't use new_image, use new_image_id" )

    @property
    def new_aligned_image( self ):
        raise RuntimeError( "aligned images as Image properties are deprecated" )

    @new_aligned_image.setter
    def new_aligned_image( self ):
        raise RuntimeError( "aligned images as Image properties are deprecated" )

    @property
    def ref_aligned_image( self ):
        raise RuntimeError( "aligned images as Image properties are deprecated" )

    @ref_aligned_image.setter
    def ref_aligned_image( self, val ):
        raise RuntimeError( "aligned images as Image properties are deprecated" )

    @property
    def sources( self ):
        raise RuntimeError( f"Image.sources is deprecated, don't use it" )

    @sources.setter
    def sources( self, val ):
        raise RuntimeError( f"Image.sources is deprecated, don't use it" )

    @property
    def psf( self ):
        raise RuntimeError( f"Image.psf is deprecated, don't use it" )

    @psf.setter
    def psf( self, val ):
        raise RuntimeError( f"Image.psf is deprecated, don't use it" )

    @property
    def bg( self ):
        raise RuntimeError( f"Image.bg is deprecated, don't use it" )

    @bg.setter
    def bg( self, val ):
        raise RuntimeError( f"Image.bg is deprecated, don't use it" )

    @property
    def wcs( self ):
        raise RuntimeError( f"Image.wcs is deprecated, don't use it" )

    @wcs.setter
    def wcs( self, val ):
        raise RuntimeError( f"Image.wcs is deprecated, don't use it" )

    @property
    def zp( self ):
        raise RuntimeError( f"Image.zp is deprecated, don't use it" )

    @zp.setter
    def zp( self, val ):
        raise RuntimeError( f"Image.zp is deprecated, don't use it" )

    @property
    def _aligner( self ):
        raise RuntimeError( f"Image._aligner is deprecated, don't use it" )

    @_aligner.setter
    def _aligner( self, val ):
        raise RuntimeError( f"Image._aligner is deprecated, don't use it" )

    @property
    def _aligned_images( self ):
        raise RuntimeError( f"Image._aligned_images is deprecated, don't use it" )

    @_aligned_images.setter
    def _aligned_images( self, val ):
        raise RuntimeError( f"Image._aligned_images is deprecated, don't use it" )

    @property
    def aligned_images( self ):
        raise RuntimeError( f"Image.aligned_images is deprecated, don't use it" )

    @aligned_images.setter
    def aligned_images( self, val ):
        raise RuntimeError( f"Image.aligned_images is deprecated, don't use it" )

    @property
    def get_psf( self ):
        raise RuntimeError( f"Image.get_psf is deprecated, don't use it" )

    @get_psf.setter
    def get_psf( self, val ):
        raise RuntimeError( f"Image.get_psf is deprecated, don't use it" )

    @property
    def get_wcs( self ):
        raise RuntimeError( f"Image.get_wcs is deprecated, don't use it" )

    @get_wcs.setter
    def get_wcs( self, val ):
        raise RuntimeError( f"Image.get_wcs is deprecated, don't use it" )



if __name__ == '__main__':
    SCLogger.warning( "Running image.py doesn't actually do anything." )

